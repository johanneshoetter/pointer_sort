{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mögliche Labels:\n",
    "- eindeutige Positionierung der Zielspalte\n",
    "- mehrfache Positionierungen der Zielspalten sortiert nach derer Kosinus-Ähnlichkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import WordEmbedding, load_word_emb\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_config = {\n",
    "    'data_dir': 'data/glove',\n",
    "    'word2idx_path': 'word2idx.json',\n",
    "    'usedwordemb_path': 'usedwordemb.npy'\n",
    "}\n",
    "w2v = WordEmbedding(load_word_emb(w2v_config['data_dir'], \n",
    "                                  w2v_config['word2idx_path'],\n",
    "                                  w2v_config['usedwordemb_path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>header</th>\n",
       "      <th>fits_1toN</th>\n",
       "      <th>fits_1to0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Date, Time, ACC Team, Big Ten Team, Location,...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Institution, Wins, Loss, Home Wins, Home Loss...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Pick #, MLS Team, Player, Position, Affiliation]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[DVD title, Number of Episodes, Region 2, Regi...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Year, Coach, Crew, Record, Win %]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              header fits_1toN fits_1to0\n",
       "0  [Date, Time, ACC Team, Big Ten Team, Location,...                    \n",
       "1  [Institution, Wins, Loss, Home Wins, Home Loss...                    \n",
       "2  [Pick #, MLS Team, Player, Position, Affiliation]                    \n",
       "3  [DVD title, Number of Episodes, Region 2, Regi...                    \n",
       "4                 [Year, Coach, Crew, Record, Win %]                    "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load table schemata\n",
    "with open('data/wikisql/tables.jsonl') as file:\n",
    "    table_schemata = pd.DataFrame([json.loads(line) for line in file.readlines()])\n",
    "table_schemata['fits_1toN'] = ''\n",
    "table_schemata['fits_1to0'] = ''\n",
    "table_schemata['header'] = table_schemata['header'].apply(lambda x: '<|>'.join(x)) # needed to drop duplicates\n",
    "table_schemata = table_schemata[['header', 'fits_1toN', 'fits_1to0']].drop_duplicates().reset_index(drop=True)\n",
    "table_schemata['header'] = table_schemata['header'].apply(lambda x: x.split('<|>')) # rebuild original state\n",
    "table_schemata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table_schemata = table_schemata.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headers = []\n",
    "_ = [all_headers.extend(header) for header in table_schemata['header'].values]\n",
    "all_headers = list(set(all_headers))\n",
    "random.shuffle(all_headers)\n",
    "candidates = [column \\\n",
    "              .replace('/', ' ') \\\n",
    "              .replace('_', ' ') \\\n",
    "              for column in all_headers[:5000]]\n",
    "cache = {candidate: np.mean([w2v(word.lower()) for word in candidate.split()], axis=0) for candidate in candidates}\n",
    "if cache.get(''):\n",
    "    del cache['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_1to1(header, cache, threshold=0.8):\n",
    "    eps = 0.001\n",
    "    embs = [np.mean([w2v(word.lower()) for word in col.split()], axis=0) for col in header]\n",
    "    fits = defaultdict(list)\n",
    "    for candidate, embedding in cache.items():\n",
    "        try:\n",
    "            vectors = [embedding] + embs\n",
    "            similarity = cosine_similarity(vectors)[0][1:]\n",
    "            max_sim = np.max(similarity)\n",
    "            if abs(max_sim + eps) >= 1.0 or max_sim < threshold:\n",
    "                continue\n",
    "            max_pos = np.argmax(similarity)\n",
    "\n",
    "            # append the best candidate with its similarity to the header\n",
    "            # to the lists of candidates for the given header\n",
    "            # i.e. fits['ACC Team'] -> [('Team', 0.75), ('Coach', 0.64), (<new candidate>, <similarity>)]\n",
    "            fits[header[max_pos]].append((candidate, max_sim))\n",
    "        except:\n",
    "            continue\n",
    "    return fits    \n",
    "\n",
    "def calculate_1t0(header, cache, threshold=0.6):\n",
    "    embs = [np.mean([w2v(word.lower()) for word in col.split()], axis=0) for col in header]\n",
    "    fits = defaultdict(list)\n",
    "    for candidate, embedding in cache.items():\n",
    "        try:\n",
    "            vectors = [embedding] + embs\n",
    "            similarity = cosine_similarity(vectors)[0][1:]\n",
    "            max_sim = np.max(similarity)\n",
    "            # if the candidate is not similar to any of the columns, append it to the fits\n",
    "            if max_sim < threshold:\n",
    "                min_sim = np.min(similarity)\n",
    "                min_pos = np.argmin(similarity)\n",
    "                fits[header[min_pos]].append((candidate, min_sim))\n",
    "        except:\n",
    "            continue\n",
    "    return fits\n",
    "\n",
    "def reduce_fits(good_fits, increase_similarity=True, lower_threshold=0.9, upper_threshold=0.3):\n",
    "    best_fits = {}\n",
    "    for column, candidate_tuples in good_fits.items():\n",
    "        best_candidate = sorted(candidate_tuples, key=lambda x: x[1], reverse=True if increase_similarity else False)[0]\n",
    "        similarity = best_candidate[1]\n",
    "        if increase_similarity:\n",
    "            if similarity > lower_threshold:\n",
    "                best_fits[column] = best_candidate[0]\n",
    "        else:\n",
    "            if similarity < upper_threshold:\n",
    "                best_fits[column] = best_candidate[0]\n",
    "    return best_fits\n",
    "\n",
    "def expand_targets(header, cache, fits_1to1, threshold=0.7):\n",
    "    fits_1toN = fits_1to1.copy()\n",
    "    for source_col, target_col in fits_1to1.items():\n",
    "        other_cols = header[:header.index(target_col)] + header[header.index(target_col) + 1:]\n",
    "        embs = [np.mean([w2v(word.lower()) for word in col.split()], axis=0) for col in other_cols]\n",
    "        vectors = embs + [cache[source_col]]\n",
    "        similarity = cosine_similarity(vectors)[0][1:]\n",
    "        indexed_similarity = sorted(enumerate(similarity), key=lambda item: item[1], reverse=True)\n",
    "        for idx, sim in indexed_similarity:\n",
    "            if sim > threshold:\n",
    "                fits_1toN[source_col] += '<|>{}'.format(other_cols[idx])\n",
    "    return fits_1toN\n",
    "\n",
    "for idx, row in tqdm(table_schemata.iterrows(), total=len(table_schemata)):\n",
    "    header = row['header']\n",
    "    fits_1to1 = calculate_1to1(header, cache)\n",
    "    fits_1to1 = reduce_fits(fits_1to1, increase_similarity=True)\n",
    "    \n",
    "    fits_1to0 = calculate_1t0(header, cache)\n",
    "    fits_1to0 = reduce_fits(fits_1to0, increase_similarity=False)\n",
    "    \n",
    "    # swap order from target_col:sourc_col to source_col: target_col\n",
    "    # implementation originally done in the target_col:source_col format\n",
    "    # as this is easier to handle for candidate reduction\n",
    "    fits_1to1 = {value: key for key, value in fits_1to1.items()}\n",
    "    fits_1to0 = {value: key for key, value in fits_1to0.items()}\n",
    "    \n",
    "    fits_1toN = expand_targets(header, cache, fits_1to1, threshold=0.75)\n",
    "    \n",
    "    row['fits_1toN'] = fits_1toN\n",
    "    row['fits_1to0'] = fits_1to0\n",
    "    table_schemata.loc[idx] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_schemata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that have not been tested\n",
    "table_schemata.drop(table_schemata.loc[(table_schemata['fits_1toN'] == '') & \\\n",
    "                                       (table_schemata['fits_1to0'] == '')].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_1toN = []\n",
    "rows_1to0 = []\n",
    "total_rows = []\n",
    "ratio_1to0_1toN = 0.2 # relative ratio of how many 1to0 cases exist in comparison to 1toN cases\n",
    "for idx, ts_row in tqdm(table_schemata.iterrows()):\n",
    "    header, fits_1toN = ts_row[['header', 'fits_1toN']]\n",
    "    num_words_in_header = len((' '.join(header)).split(' '))\n",
    "    if num_words_in_header > 30:\n",
    "        continue\n",
    "    for source_col, target_cols in fits_1toN.items(): \n",
    "        if 'k {\\math' in source_col:\n",
    "            continue\n",
    "        seq_row = {\n",
    "            'source_col': source_col,\n",
    "            'input_cols': '<|>'.join(header),\n",
    "            'target_cols': '<|>'.join(target_cols.split('<|>')) # first create a list from the joined \n",
    "                        # target cols, then concatenate them (needs to be done to serialize them correctly)\n",
    "        }\n",
    "        rows_1toN.append(seq_row)\n",
    "    for source_col, _ in fits_1to0.items(): \n",
    "        if 'k {\\math' in source_col:\n",
    "            continue\n",
    "        seq_row = {\n",
    "            'source_col': source_col,\n",
    "            'input_cols': '<|>'.join(header),\n",
    "            'target_cols': '<NONE>'\n",
    "        }\n",
    "        rows_1to0.append(seq_row)\n",
    "amount_1to0 = int(len(rows_1toN) * ratio_1to0_1toN)\n",
    "rows_1toN, rows_1to0 = shuffle(rows_1toN), shuffle(rows_1to0)[:amount_1to0]\n",
    "\n",
    "total_rows = shuffle(rows_1toN + rows_1to0)\n",
    "\n",
    "total_rows = pd.DataFrame(total_rows)\n",
    "total_rows.to_csv('data/training/schema_matching_raw_1toN.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_1to0 = len(total_rows[total_rows['target_cols'] == '<NONE>'])\n",
    "num_1toN = len(total_rows[(total_rows['target_cols'].str.contains('<|>')) &\n",
    "                          (total_rows['target_cols'] != '<NONE>')])\n",
    "num_1to1 = len(total_rows) - num_1toN - num_1to0\n",
    "num_1to0, num_1to1, num_1toN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorgehen:\n",
    "1. für jede Tabelle eine Spalte finden, die aus Quellschema sein könnte -> die als Eingabe  \n",
    "Format: Quellspalte -> [Zielspalte]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
