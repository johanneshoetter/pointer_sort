{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import AlphabetSortingDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "params = {\n",
    "    # Data\n",
    "    'batch_size': 512,\n",
    "    'shuffle': True,\n",
    "    'nof_workers': 0, # must stay at 0\n",
    "    #Train\n",
    "    'nof_epoch': 1000,\n",
    "    'lr': 0.0001,\n",
    "    # GPU\n",
    "    'gpu': True,\n",
    "    # Network\n",
    "    'embedding_size': 300,\n",
    "    'hiddens': 512,\n",
    "    'nof_lstms': 8,\n",
    "    'dropout': 0.3,\n",
    "    'bidir': False # True not working right now\n",
    "}\n",
    "\n",
    "dataset = AlphabetSortingDataset(100000, min_len=4, max_len=4)\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=params['batch_size'],\n",
    "                        shuffle=params['shuffle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU, 1 devices.\n"
     ]
    }
   ],
   "source": [
    "if params['gpu'] and torch.cuda.is_available():\n",
    "    USE_CUDA = True\n",
    "    print('Using GPU, %i devices.' % torch.cuda.device_count())\n",
    "else:\n",
    "    USE_CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder class for Pointer-Net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 n_layers,\n",
    "                 dropout,\n",
    "                 bidir):\n",
    "        \"\"\"\n",
    "        Initiate Encoder\n",
    "        :param Tensor embedding_dim: Number of embbeding channels\n",
    "        :param int hidden_dim: Number of hidden units for the LSTM\n",
    "        :param int n_layers: Number of layers for LSTMs\n",
    "        :param float dropout: Float between 0-1\n",
    "        :param bool bidir: Bidirectional\n",
    "        \"\"\"\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_dim = hidden_dim//2 if bidir else hidden_dim\n",
    "        self.n_layers = n_layers*2 if bidir else n_layers\n",
    "        self.bidir = bidir\n",
    "        self.lstm = nn.LSTM(embedding_dim,\n",
    "                            self.hidden_dim,\n",
    "                            n_layers,\n",
    "                            dropout=dropout,\n",
    "                            bidirectional=bidir)\n",
    "\n",
    "        # Used for propagating .cuda() command\n",
    "        self.h0 = Parameter(torch.zeros(1), requires_grad=False)\n",
    "        self.c0 = Parameter(torch.zeros(1), requires_grad=False)\n",
    "\n",
    "    def forward(self, embedded_inputs,\n",
    "                hidden):\n",
    "        \"\"\"\n",
    "        Encoder - Forward-pass\n",
    "        :param Tensor embedded_inputs: Embedded inputs of Pointer-Net\n",
    "        :param Tensor hidden: Initiated hidden units for the LSTMs (h, c)\n",
    "        :return: LSTMs outputs and hidden units (h, c)\n",
    "        \"\"\"\n",
    "\n",
    "        embedded_inputs = embedded_inputs.permute(1, 0, 2)\n",
    "\n",
    "        outputs, hidden = self.lstm(embedded_inputs, hidden)\n",
    "\n",
    "        return outputs.permute(1, 0, 2), hidden\n",
    "\n",
    "    def init_hidden(self, embedded_inputs):\n",
    "        \"\"\"\n",
    "        Initiate hidden units\n",
    "        :param Tensor embedded_inputs: The embedded input of Pointer-NEt\n",
    "        :return: Initiated hidden units for the LSTMs (h, c)\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = embedded_inputs.size(0)\n",
    "\n",
    "        # Reshaping (Expanding)\n",
    "        h0 = self.h0.unsqueeze(0).unsqueeze(0).repeat(self.n_layers,\n",
    "                                                      batch_size,\n",
    "                                                      self.hidden_dim)\n",
    "        c0 = self.h0.unsqueeze(0).unsqueeze(0).repeat(self.n_layers,\n",
    "                                                      batch_size,\n",
    "                                                      self.hidden_dim)\n",
    "\n",
    "        return h0, c0\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention model for Pointer-Net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_dim,\n",
    "                 hidden_dim):\n",
    "        \"\"\"\n",
    "        Initiate Attention\n",
    "        :param int input_dim: Input's diamention\n",
    "        :param int hidden_dim: Number of hidden units in the attention\n",
    "        \"\"\"\n",
    "\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.input_linear = nn.Linear(input_dim, hidden_dim)\n",
    "        self.context_linear = nn.Conv1d(input_dim, hidden_dim, 1, 1)\n",
    "        self.V = Parameter(torch.FloatTensor(hidden_dim), requires_grad=True)\n",
    "        self._inf = Parameter(torch.FloatTensor([float('-inf')]), requires_grad=False)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        # Initialize vector V\n",
    "        nn.init.uniform(self.V, -1, 1)\n",
    "\n",
    "    def forward(self, input,\n",
    "                context,\n",
    "                mask):\n",
    "        \"\"\"\n",
    "        Attention - Forward-pass\n",
    "        :param Tensor input: Hidden state h\n",
    "        :param Tensor context: Attention context\n",
    "        :param ByteTensor mask: Selection mask\n",
    "        :return: tuple of - (Attentioned hidden state, Alphas)\n",
    "        \"\"\"\n",
    "\n",
    "        # (batch, hidden_dim, seq_len)\n",
    "        inp = self.input_linear(input).unsqueeze(2).expand(-1, -1, context.size(1))\n",
    "\n",
    "        # (batch, hidden_dim, seq_len)\n",
    "        context = context.permute(0, 2, 1)\n",
    "        ctx = self.context_linear(context)\n",
    "\n",
    "        # (batch, 1, hidden_dim)\n",
    "        V = self.V.unsqueeze(0).expand(context.size(0), -1).unsqueeze(1)\n",
    "\n",
    "        # (batch, seq_len)\n",
    "        att = torch.bmm(V, self.tanh(inp + ctx)).squeeze(1)\n",
    "        if len(att[mask]) > 0:\n",
    "            att[mask] = self.inf[mask]\n",
    "        alpha = self.softmax(att)\n",
    "\n",
    "        hidden_state = torch.bmm(ctx, alpha.unsqueeze(2)).squeeze(2)\n",
    "\n",
    "        return hidden_state, alpha\n",
    "\n",
    "    def init_inf(self, mask_size):\n",
    "        self.inf = self._inf.unsqueeze(1).expand(*mask_size)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder model for Pointer-Net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim,\n",
    "                 hidden_dim):\n",
    "        \"\"\"\n",
    "        Initiate Decoder\n",
    "        :param int embedding_dim: Number of embeddings in Pointer-Net\n",
    "        :param int hidden_dim: Number of hidden units for the decoder's RNN\n",
    "        \"\"\"\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.input_to_hidden = nn.Linear(embedding_dim, 4 * hidden_dim)\n",
    "        self.hidden_to_hidden = nn.Linear(hidden_dim, 4 * hidden_dim)\n",
    "        self.hidden_out = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.att = Attention(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Used for propagating .cuda() command\n",
    "        self.mask = Parameter(torch.ones(1), requires_grad=False)\n",
    "        self.runner = Parameter(torch.zeros(1), requires_grad=False)\n",
    "\n",
    "    def forward(self, embedded_inputs,\n",
    "                decoder_input,\n",
    "                hidden,\n",
    "                context):\n",
    "        \"\"\"\n",
    "        Decoder - Forward-pass\n",
    "        :param Tensor embedded_inputs: Embedded inputs of Pointer-Net\n",
    "        :param Tensor decoder_input: First decoder's input\n",
    "        :param Tensor hidden: First decoder's hidden states\n",
    "        :param Tensor context: Encoder's outputs\n",
    "        :return: (Output probabilities, Pointers indices), last hidden state\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = embedded_inputs.size(0)\n",
    "        input_length = embedded_inputs.size(1)\n",
    "\n",
    "        # (batch, seq_len)\n",
    "        mask = self.mask.repeat(input_length).unsqueeze(0).repeat(batch_size, 1)\n",
    "        self.att.init_inf(mask.size())\n",
    "\n",
    "        # Generating arang(input_length), broadcasted across batch_size\n",
    "        runner = self.runner.repeat(input_length)\n",
    "        for i in range(input_length):\n",
    "            runner.data[i] = i\n",
    "        runner = runner.unsqueeze(0).expand(batch_size, -1).long()\n",
    "\n",
    "        outputs = []\n",
    "        pointers = []\n",
    "\n",
    "        def step(x, hidden):\n",
    "            \"\"\"\n",
    "            Recurrence step function\n",
    "            :param Tensor x: Input at time t\n",
    "            :param tuple(Tensor, Tensor) hidden: Hidden states at time t-1\n",
    "            :return: Hidden states at time t (h, c), Attention probabilities (Alpha)\n",
    "            \"\"\"\n",
    "\n",
    "            # Regular LSTM\n",
    "            h, c = hidden\n",
    "\n",
    "            gates = self.input_to_hidden(x) + self.hidden_to_hidden(h)\n",
    "            input, forget, cell, out = gates.chunk(4, 1)\n",
    "\n",
    "            input = F.sigmoid(input)\n",
    "            forget = F.sigmoid(forget)\n",
    "            cell = F.tanh(cell)\n",
    "            out = F.sigmoid(out)\n",
    "\n",
    "            c_t = (forget * c) + (input * cell)\n",
    "            h_t = out * F.tanh(c_t)\n",
    "\n",
    "            # Attention section\n",
    "            hidden_t, output = self.att(h_t, context, torch.eq(mask, 0))\n",
    "            hidden_t = F.tanh(self.hidden_out(torch.cat((hidden_t, h_t), 1)))\n",
    "\n",
    "            return hidden_t, c_t, output\n",
    "\n",
    "        # Recurrence loop\n",
    "        for _ in range(input_length):\n",
    "            h_t, c_t, outs = step(decoder_input, hidden)\n",
    "            hidden = (h_t, c_t)\n",
    "\n",
    "            # Masking selected inputs\n",
    "            masked_outs = outs * mask\n",
    "\n",
    "            # Get maximum probabilities and indices\n",
    "            max_probs, indices = masked_outs.max(1)\n",
    "            one_hot_pointers = (runner == indices.unsqueeze(1).expand(-1, outs.size()[1])).float()\n",
    "\n",
    "            # Update mask to ignore seen indices\n",
    "            mask  = mask * (1 - one_hot_pointers)\n",
    "\n",
    "            # Get embedded inputs by max indices\n",
    "            embedding_mask = one_hot_pointers.unsqueeze(2).expand(-1, -1, self.embedding_dim).byte()\n",
    "            decoder_input = embedded_inputs[embedding_mask.data].view(batch_size, self.embedding_dim)\n",
    "\n",
    "            outputs.append(outs.unsqueeze(0))\n",
    "            pointers.append(indices.unsqueeze(1))\n",
    "\n",
    "        outputs = torch.cat(outputs).permute(1, 0, 2)\n",
    "        pointers = torch.cat(pointers, 1)\n",
    "\n",
    "        return (outputs, pointers), hidden\n",
    "\n",
    "\n",
    "class PointerNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Pointer-Net\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim,\n",
    "                 hidden_dim,\n",
    "                 lstm_layers,\n",
    "                 dropout,\n",
    "                 bidir=False):\n",
    "        \"\"\"\n",
    "        Initiate Pointer-Net\n",
    "        :param int embedding_dim: Number of embbeding channels\n",
    "        :param int hidden_dim: Encoders hidden units\n",
    "        :param int lstm_layers: Number of layers for LSTMs\n",
    "        :param float dropout: Float between 0-1\n",
    "        :param bool bidir: Bidirectional\n",
    "        \"\"\"\n",
    "\n",
    "        super(PointerNet, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.bidir = bidir\n",
    "        self.embedding = nn.Linear(embedding_dim, embedding_dim) #nn.Linear(2, embedding_dim)\n",
    "        self.encoder = Encoder(embedding_dim,\n",
    "                               hidden_dim,\n",
    "                               lstm_layers,\n",
    "                               dropout,\n",
    "                               bidir)\n",
    "        self.decoder = Decoder(embedding_dim, hidden_dim)\n",
    "        self.decoder_input0 = Parameter(torch.FloatTensor(embedding_dim), requires_grad=False)\n",
    "\n",
    "        # Initialize decoder_input0\n",
    "        nn.init.uniform(self.decoder_input0, -1, 1)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        PointerNet - Forward-pass\n",
    "        :param Tensor inputs: Input sequence\n",
    "        :return: Pointers probabilities and indices\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = inputs.size(0)\n",
    "        input_length = inputs.size(1)\n",
    "\n",
    "        decoder_input0 = self.decoder_input0.unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "        inputs = inputs.view(batch_size * input_length, -1)\n",
    "        embedded_inputs = self.embedding(inputs).view(batch_size, input_length, -1)\n",
    "\n",
    "        encoder_hidden0 = self.encoder.init_hidden(embedded_inputs)\n",
    "        encoder_outputs, encoder_hidden = self.encoder(embedded_inputs,\n",
    "                                                       encoder_hidden0)\n",
    "        if self.bidir:\n",
    "            decoder_hidden0 = (torch.cat(encoder_hidden[0][-2:], dim=-1),\n",
    "                               torch.cat(encoder_hidden[1][-2:], dim=-1))\n",
    "        else:\n",
    "            decoder_hidden0 = (encoder_hidden[0][-1],\n",
    "                               encoder_hidden[1][-1])\n",
    "        (outputs, pointers), decoder_hidden = self.decoder(embedded_inputs,\n",
    "                                                           decoder_input0,\n",
    "                                                           decoder_hidden0,\n",
    "                                                           encoder_outputs)\n",
    "\n",
    "        return  outputs, pointers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PointerNet(params['embedding_size'],\n",
    "                   params['hiddens'],\n",
    "                   params['nof_lstms'],\n",
    "                   params['dropout'],\n",
    "                   params['bidir'])\n",
    "\n",
    "if USE_CUDA:\n",
    "    model.cuda()\n",
    "    net = torch.nn.DataParallel(model, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "CCE = torch.nn.CrossEntropyLoss()\n",
    "model_optim = optim.Adam(filter(lambda p: p.requires_grad,\n",
    "                                model.parameters()),\n",
    "                                 lr=params['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000: 100%|██████████████████████████████████████| 196/196 [00:22<00:00,  8.86Batch/s, loss=1.3025537729263306]\n",
      "Epoch 2/1000: 100%|██████████████████████████████████████| 196/196 [00:21<00:00,  9.15Batch/s, loss=1.1566112041473389]\n",
      "Epoch 3/1000: 100%|██████████████████████████████████████| 196/196 [00:21<00:00,  9.19Batch/s, loss=1.1035975217819214]\n",
      "Epoch 4/1000: 100%|██████████████████████████████████████| 196/196 [00:21<00:00,  9.27Batch/s, loss=1.0654791593551636]\n",
      "Epoch 5/1000: 100%|██████████████████████████████████████| 196/196 [00:21<00:00,  9.29Batch/s, loss=1.0273122787475586]\n",
      "Epoch 6/1000: 100%|██████████████████████████████████████| 196/196 [00:21<00:00,  9.06Batch/s, loss=0.8592767715454102]\n",
      "Epoch 7/1000: 100%|██████████████████████████████████████| 196/196 [00:21<00:00,  9.17Batch/s, loss=0.8141610026359558]\n",
      "Epoch 8/1000: 100%|██████████████████████████████████████| 196/196 [00:21<00:00,  9.29Batch/s, loss=0.7907483577728271]\n",
      "Epoch 9/1000: 100%|███████████████████████████████████████| 196/196 [00:21<00:00,  9.04Batch/s, loss=0.800970196723938]\n",
      "Epoch 10/1000: 100%|██████████████████████████████████████| 196/196 [00:21<00:00,  9.15Batch/s, loss=0.773605227470398]\n",
      "Epoch 11/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.09Batch/s, loss=0.7783305048942566]\n",
      "Epoch 12/1000: 100%|█████████████████████████████████████| 196/196 [00:20<00:00,  9.34Batch/s, loss=0.7851947546005249]\n",
      "Epoch 13/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.05Batch/s, loss=0.7798829078674316]\n",
      "Epoch 14/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.04Batch/s, loss=0.7543980479240417]\n",
      "Epoch 15/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.07Batch/s, loss=0.7659016847610474]\n",
      "Epoch 16/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.23Batch/s, loss=0.7661234140396118]\n",
      "Epoch 17/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.25Batch/s, loss=0.7783637046813965]\n",
      "Epoch 18/1000: 100%|██████████████████████████████████████| 196/196 [00:20<00:00,  9.38Batch/s, loss=0.774522066116333]\n",
      "Epoch 19/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.13Batch/s, loss=0.7569974660873413]\n",
      "Epoch 20/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.11Batch/s, loss=0.7549552321434021]\n",
      "Epoch 21/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.17Batch/s, loss=0.7620594501495361]\n",
      "Epoch 22/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.10Batch/s, loss=0.7591709494590759]\n",
      "Epoch 23/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.15Batch/s, loss=0.7614744305610657]\n",
      "Epoch 24/1000: 100%|█████████████████████████████████████| 196/196 [00:20<00:00,  9.35Batch/s, loss=0.7589422464370728]\n",
      "Epoch 25/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.22Batch/s, loss=0.7690137624740601]\n",
      "Epoch 26/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.22Batch/s, loss=0.8120764493942261]\n",
      "Epoch 27/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.09Batch/s, loss=0.7635785937309265]\n",
      "Epoch 28/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.24Batch/s, loss=0.7695006132125854]\n",
      "Epoch 29/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.29Batch/s, loss=0.7757126092910767]\n",
      "Epoch 30/1000: 100%|██████████████████████████████████████| 196/196 [00:21<00:00,  9.02Batch/s, loss=0.755747377872467]\n",
      "Epoch 31/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.31Batch/s, loss=0.7662333250045776]\n",
      "Epoch 32/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  8.95Batch/s, loss=0.7734490633010864]\n",
      "Epoch 33/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.19Batch/s, loss=0.7742641568183899]\n",
      "Epoch 34/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.04Batch/s, loss=0.7593127489089966]\n",
      "Epoch 35/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.19Batch/s, loss=0.7618573904037476]\n",
      "Epoch 36/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.16Batch/s, loss=0.7529717683792114]\n",
      "Epoch 37/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.15Batch/s, loss=0.7659703493118286]\n",
      "Epoch 38/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.16Batch/s, loss=0.7576521039009094]\n",
      "Epoch 39/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.00Batch/s, loss=0.7773395776748657]\n",
      "Epoch 40/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.14Batch/s, loss=0.7564505338668823]\n",
      "Epoch 41/1000: 100%|██████████████████████████████████████| 196/196 [00:21<00:00,  9.18Batch/s, loss=0.762455940246582]\n",
      "Epoch 42/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.19Batch/s, loss=0.7697089910507202]\n",
      "Epoch 43/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.06Batch/s, loss=0.7680636644363403]\n",
      "Epoch 44/1000: 100%|██████████████████████████████████████| 196/196 [00:21<00:00,  9.19Batch/s, loss=0.762541651725769]\n",
      "Epoch 45/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.01Batch/s, loss=0.7535151243209839]\n",
      "Epoch 46/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.21Batch/s, loss=0.7722909450531006]\n",
      "Epoch 47/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.25Batch/s, loss=0.7624586224555969]\n",
      "Epoch 48/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.07Batch/s, loss=0.7597631216049194]\n",
      "Epoch 49/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.08Batch/s, loss=0.7597487568855286]\n",
      "Epoch 50/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  8.98Batch/s, loss=0.7736020088195801]\n",
      "Epoch 51/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.07Batch/s, loss=0.7630181312561035]\n",
      "Epoch 52/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.18Batch/s, loss=0.7593108415603638]\n",
      "Epoch 53/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.05Batch/s, loss=0.7921139597892761]\n",
      "Epoch 54/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.18Batch/s, loss=0.7583878040313721]\n",
      "Epoch 55/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.15Batch/s, loss=0.7561695575714111]\n",
      "Epoch 56/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.16Batch/s, loss=0.7592884302139282]\n",
      "Epoch 57/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.06Batch/s, loss=0.7655439972877502]\n",
      "Epoch 58/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.04Batch/s, loss=0.7718311548233032]\n",
      "Epoch 59/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.22Batch/s, loss=0.7790359258651733]\n",
      "Epoch 60/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.09Batch/s, loss=0.7562029361724854]\n",
      "Epoch 61/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.13Batch/s, loss=0.7687072157859802]\n",
      "Epoch 62/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.16Batch/s, loss=0.7772527933120728]\n",
      "Epoch 63/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.07Batch/s, loss=0.7858661413192749]\n",
      "Epoch 64/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.00Batch/s, loss=0.7723245620727539]\n",
      "Epoch 65/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.16Batch/s, loss=0.7626725435256958]\n",
      "Epoch 66/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.11Batch/s, loss=0.7624187469482422]\n",
      "Epoch 67/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.12Batch/s, loss=0.7530592083930969]\n",
      "Epoch 68/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.07Batch/s, loss=0.7780437469482422]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.04Batch/s, loss=0.7468768954277039]\n",
      "Epoch 70/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.31Batch/s, loss=0.7723820209503174]\n",
      "Epoch 71/1000: 100%|██████████████████████████████████████| 196/196 [00:21<00:00,  9.19Batch/s, loss=0.762597918510437]\n",
      "Epoch 72/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.08Batch/s, loss=0.7655274868011475]\n",
      "Epoch 73/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  8.98Batch/s, loss=0.7778753042221069]\n",
      "Epoch 74/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  8.97Batch/s, loss=0.7775134444236755]\n",
      "Epoch 75/1000: 100%|███████████████████████████████████████| 196/196 [00:21<00:00,  8.91Batch/s, loss=0.78975909948349]\n",
      "Epoch 76/1000: 100%|███████████████████████████████████████| 196/196 [00:21<00:00,  9.13Batch/s, loss=0.77491295337677]\n",
      "Epoch 77/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.11Batch/s, loss=0.7748664617538452]\n",
      "Epoch 78/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.14Batch/s, loss=0.7675132751464844]\n",
      "Epoch 79/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.00Batch/s, loss=0.7875357866287231]\n",
      "Epoch 80/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.04Batch/s, loss=0.7624222040176392]\n",
      "Epoch 81/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  8.94Batch/s, loss=0.7827311754226685]\n",
      "Epoch 82/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.01Batch/s, loss=0.7718471884727478]\n",
      "Epoch 83/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.06Batch/s, loss=0.7686687707901001]\n",
      "Epoch 84/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.21Batch/s, loss=0.7749285697937012]\n",
      "Epoch 85/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  8.99Batch/s, loss=0.7779940962791443]\n",
      "Epoch 86/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.04Batch/s, loss=0.7717975378036499]\n",
      "Epoch 87/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.28Batch/s, loss=0.7468119263648987]\n",
      "Epoch 88/1000: 100%|█████████████████████████████████████| 196/196 [00:20<00:00,  9.35Batch/s, loss=0.7623805999755859]\n",
      "Epoch 89/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.09Batch/s, loss=0.7608565092086792]\n",
      "Epoch 90/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  8.95Batch/s, loss=0.7502907514572144]\n",
      "Epoch 91/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.23Batch/s, loss=0.7722427845001221]\n",
      "Epoch 92/1000: 100%|██████████████████████████████████████| 196/196 [00:21<00:00,  9.18Batch/s, loss=0.768649160861969]\n",
      "Epoch 93/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.15Batch/s, loss=0.7624187469482422]\n",
      "Epoch 94/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.25Batch/s, loss=0.7717944383621216]\n",
      "Epoch 95/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  8.99Batch/s, loss=0.7662147283554077]\n",
      "Epoch 96/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.06Batch/s, loss=0.7702391147613525]\n",
      "Epoch 97/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.10Batch/s, loss=0.7655425667762756]\n",
      "Epoch 98/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.13Batch/s, loss=0.7593203783035278]\n",
      "Epoch 99/1000: 100%|██████████████████████████████████████| 196/196 [00:21<00:00,  9.27Batch/s, loss=0.765458345413208]\n",
      "Epoch 100/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  8.98Batch/s, loss=0.7653645277023315]\n",
      "Epoch 101/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.18Batch/s, loss=0.7655476331710815]\n",
      "Epoch 102/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.16Batch/s, loss=0.7592940926551819]\n",
      "Epoch 103/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.03Batch/s, loss=0.7592937350273132]\n",
      "Epoch 104/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  8.99Batch/s, loss=0.7780437469482422]\n",
      "Epoch 105/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.13Batch/s, loss=0.7568908929824829]\n",
      "Epoch 106/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.15Batch/s, loss=0.7678996324539185]\n",
      "Epoch 107/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.24Batch/s, loss=0.7615579962730408]\n",
      "Epoch 108/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.13Batch/s, loss=0.7624166011810303]\n",
      "Epoch 109/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.12Batch/s, loss=0.7686644792556763]\n",
      "Epoch 110/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.23Batch/s, loss=0.7686818242073059]\n",
      "Epoch 111/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.08Batch/s, loss=0.7592976689338684]\n",
      "Epoch 112/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.19Batch/s, loss=0.7948962450027466]\n",
      "Epoch 113/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.17Batch/s, loss=0.759294867515564]\n",
      "Epoch 114/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  8.98Batch/s, loss=0.7702312469482422]\n",
      "Epoch 115/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.07Batch/s, loss=0.7749190330505371]\n",
      "Epoch 116/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.03Batch/s, loss=0.7639828324317932]\n",
      "Epoch 117/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.19Batch/s, loss=0.7764809131622314]\n",
      "Epoch 118/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.14Batch/s, loss=0.7661496996879578]\n",
      "Epoch 119/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.09Batch/s, loss=0.7686687707901001]\n",
      "Epoch 120/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.00Batch/s, loss=0.805713951587677]\n",
      "Epoch 121/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.14Batch/s, loss=0.7655438184738159]\n",
      "Epoch 122/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.14Batch/s, loss=0.7921060919761658]\n",
      "Epoch 123/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.12Batch/s, loss=0.7655447125434875]\n",
      "Epoch 124/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.15Batch/s, loss=0.8135267496109009]\n",
      "Epoch 125/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.02Batch/s, loss=0.756169855594635]\n",
      "Epoch 126/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.03Batch/s, loss=0.7836059331893921]\n",
      "Epoch 127/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.14Batch/s, loss=0.7593024969100952]\n",
      "Epoch 128/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.13Batch/s, loss=0.7874197363853455]\n",
      "Epoch 129/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.03Batch/s, loss=0.7530438899993896]\n",
      "Epoch 130/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  8.98Batch/s, loss=0.7655438184738159]\n",
      "Epoch 131/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.06Batch/s, loss=0.7780169248580933]\n",
      "Epoch 132/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.00Batch/s, loss=0.7577444314956665]\n",
      "Epoch 133/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.12Batch/s, loss=0.7711611986160278]\n",
      "Epoch 134/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.05Batch/s, loss=0.7811987996101379]\n",
      "Epoch 135/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.05Batch/s, loss=0.7733563184738159]\n",
      "Epoch 136/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.03Batch/s, loss=0.774919867515564]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 137/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.07Batch/s, loss=0.7796066403388977]\n",
      "Epoch 138/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.12Batch/s, loss=0.762412428855896]\n",
      "Epoch 139/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.09Batch/s, loss=0.7749530076980591]\n",
      "Epoch 140/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.03Batch/s, loss=0.7608559131622314]\n",
      "Epoch 141/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.05Batch/s, loss=0.7877832651138306]\n",
      "Epoch 142/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.15Batch/s, loss=0.7655674815177917]\n",
      "Epoch 143/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.04Batch/s, loss=0.7533819675445557]\n",
      "Epoch 144/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.15Batch/s, loss=0.7733631730079651]\n",
      "Epoch 145/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.03Batch/s, loss=0.7624073028564453]\n",
      "Epoch 146/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.07Batch/s, loss=0.7811694741249084]\n",
      "Epoch 147/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.09Batch/s, loss=0.7874189615249634]\n",
      "Epoch 148/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  8.93Batch/s, loss=0.7656477689743042]\n",
      "Epoch 149/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  8.94Batch/s, loss=0.7592943906784058]\n",
      "Epoch 150/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.05Batch/s, loss=0.7983562350273132]\n",
      "Epoch 151/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  8.97Batch/s, loss=0.764072060585022]\n",
      "Epoch 152/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.12Batch/s, loss=0.7936586737632751]\n",
      "Epoch 153/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.00Batch/s, loss=0.7781375646591187]\n",
      "Epoch 154/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.07Batch/s, loss=0.7561690211296082]\n",
      "Epoch 155/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.13Batch/s, loss=0.7655436992645264]\n",
      "Epoch 156/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.13Batch/s, loss=0.7905550599098206]\n",
      "Epoch 157/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.13Batch/s, loss=0.7874187231063843]\n",
      "Epoch 158/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  8.97Batch/s, loss=0.7624187469482422]\n",
      "Epoch 159/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.21Batch/s, loss=0.7655463218688965]\n",
      "Epoch 160/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.02Batch/s, loss=0.7689529061317444]\n",
      "Epoch 161/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.15Batch/s, loss=0.7624295949935913]\n",
      "Epoch 162/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  8.92Batch/s, loss=0.7859493494033813]\n",
      "Epoch 163/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.10Batch/s, loss=0.7874184846878052]\n",
      "Epoch 164/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.12Batch/s, loss=0.7872375249862671]\n",
      "Epoch 165/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.11Batch/s, loss=0.7595065832138062]\n",
      "Epoch 166/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.05Batch/s, loss=0.7624295949935913]\n",
      "Epoch 167/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.10Batch/s, loss=0.7749199867248535]\n",
      "Epoch 168/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.01Batch/s, loss=0.7872766256332397]\n",
      "Epoch 169/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.13Batch/s, loss=0.7803201675415039]\n",
      "Epoch 170/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.05Batch/s, loss=0.7724263072013855]\n",
      "Epoch 171/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.13Batch/s, loss=0.771308183670044]\n",
      "Epoch 172/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.13Batch/s, loss=0.7749207019805908]\n",
      "Epoch 173/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.06Batch/s, loss=0.768676221370697]\n",
      "Epoch 174/1000: 100%|████████████████████████████████████| 196/196 [00:22<00:00,  8.91Batch/s, loss=0.7499574422836304]\n",
      "Epoch 175/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.04Batch/s, loss=0.753044068813324]\n",
      "Epoch 176/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.02Batch/s, loss=0.7499187588691711]\n",
      "Epoch 177/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.21Batch/s, loss=0.7921027541160583]\n",
      "Epoch 178/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.11Batch/s, loss=0.787417471408844]\n",
      "Epoch 179/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.13Batch/s, loss=0.7624187469482422]\n",
      "Epoch 180/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.07Batch/s, loss=0.7780412435531616]\n",
      "Epoch 181/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.08Batch/s, loss=0.7748728394508362]\n",
      "Epoch 182/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  8.98Batch/s, loss=0.7796082496643066]\n",
      "Epoch 183/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.10Batch/s, loss=0.7811688780784607]\n",
      "Epoch 184/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.07Batch/s, loss=0.7780469655990601]\n",
      "Epoch 185/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.14Batch/s, loss=0.7905451655387878]\n",
      "Epoch 186/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.04Batch/s, loss=0.7827321290969849]\n",
      "Epoch 187/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.08Batch/s, loss=0.7671043872833252]\n",
      "Epoch 188/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  8.98Batch/s, loss=0.7936687469482422]\n",
      "Epoch 189/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.06Batch/s, loss=0.8062998652458191]\n",
      "Epoch 190/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.18Batch/s, loss=0.7782406806945801]\n",
      "Epoch 191/1000: 100%|████████████████████████████████████| 196/196 [00:22<00:00,  8.83Batch/s, loss=0.7842861413955688]\n",
      "Epoch 192/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.09Batch/s, loss=0.7686821818351746]\n",
      "Epoch 193/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.05Batch/s, loss=0.765544056892395]\n",
      "Epoch 194/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.02Batch/s, loss=0.7804950475692749]\n",
      "Epoch 195/1000: 100%|███████████████████████████████████████| 196/196 [00:21<00:00,  9.09Batch/s, loss=0.7686727643013]\n",
      "Epoch 196/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.14Batch/s, loss=0.7655438184738159]\n",
      "Epoch 197/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.09Batch/s, loss=0.7715705037117004]\n",
      "Epoch 198/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.16Batch/s, loss=0.7624187469482422]\n",
      "Epoch 199/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.01Batch/s, loss=0.7686687707901001]\n",
      "Epoch 200/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.23Batch/s, loss=0.7811679840087891]\n",
      "Epoch 201/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  8.97Batch/s, loss=0.7657746076583862]\n",
      "Epoch 202/1000: 100%|████████████████████████████████████| 196/196 [00:22<00:00,  8.91Batch/s, loss=0.7561688423156738]\n",
      "Epoch 203/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.06Batch/s, loss=0.7717939019203186]\n",
      "Epoch 204/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.16Batch/s, loss=0.7780083417892456]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 205/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.11Batch/s, loss=0.7592939734458923]\n",
      "Epoch 206/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.04Batch/s, loss=0.7752271890640259]\n",
      "Epoch 207/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.16Batch/s, loss=0.7764835953712463]\n",
      "Epoch 208/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.11Batch/s, loss=0.7782772183418274]\n",
      "Epoch 209/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.15Batch/s, loss=0.7592937350273132]\n",
      "Epoch 210/1000: 100%|█████████████████████████████████████| 196/196 [00:20<00:00,  9.34Batch/s, loss=0.759919285774231]\n",
      "Epoch 211/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.05Batch/s, loss=0.7686721682548523]\n",
      "Epoch 212/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.09Batch/s, loss=0.7561687231063843]\n",
      "Epoch 213/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  8.99Batch/s, loss=0.7717941999435425]\n",
      "Epoch 214/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.17Batch/s, loss=0.7780436277389526]\n",
      "Epoch 215/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.09Batch/s, loss=0.7561688423156738]\n",
      "Epoch 216/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.17Batch/s, loss=0.7780437469482422]\n",
      "Epoch 217/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.17Batch/s, loss=0.7749188542366028]\n",
      "Epoch 218/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.28Batch/s, loss=0.765544056892395]\n",
      "Epoch 219/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.14Batch/s, loss=0.7751160860061646]\n",
      "Epoch 220/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.09Batch/s, loss=0.7592938542366028]\n",
      "Epoch 221/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.10Batch/s, loss=0.7842937111854553]\n",
      "Epoch 222/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.28Batch/s, loss=0.774920642375946]\n",
      "Epoch 223/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.08Batch/s, loss=0.7717938423156738]\n",
      "Epoch 224/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.21Batch/s, loss=0.7655438780784607]\n",
      "Epoch 225/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.19Batch/s, loss=0.7717937231063843]\n",
      "Epoch 226/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.25Batch/s, loss=0.7749187350273132]\n",
      "Epoch 227/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.06Batch/s, loss=0.7938008308410645]\n",
      "Epoch 228/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.22Batch/s, loss=0.768669068813324]\n",
      "Epoch 229/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.19Batch/s, loss=0.7499187588691711]\n",
      "Epoch 230/1000: 100%|█████████████████████████████████████| 196/196 [00:21<00:00,  9.24Batch/s, loss=0.765541672706604]\n",
      "Epoch 231/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.15Batch/s, loss=0.7655437588691711]\n",
      "Epoch 232/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.17Batch/s, loss=0.7811688184738159]\n",
      "Epoch 233/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.16Batch/s, loss=0.7811688184738159]\n",
      "Epoch 234/1000: 100%|████████████████████████████████████| 196/196 [00:21<00:00,  9.15Batch/s, loss=0.7721180319786072]\n",
      "Epoch 235/1000:  37%|██████████████▏                       | 73/196 [00:07<00:13,  8.80Batch/s, loss=0.778664231300354]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f2087024a83f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m             \u001b[0mtarget_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mo\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mtarget_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ca\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-5699e2479367>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    308\u001b[0m                                                            \u001b[0mdecoder_input0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m                                                            \u001b[0mdecoder_hidden0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m                                                            encoder_outputs)\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m         \u001b[1;32mreturn\u001b[0m  \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpointers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ca\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-5699e2479367>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, embedded_inputs, decoder_input, hidden, context)\u001b[0m\n\u001b[0;32m    229\u001b[0m             \u001b[1;31m# Get maximum probabilities and indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m             \u001b[0mmax_probs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmasked_outs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m             \u001b[0mone_hot_pointers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrunner\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m             \u001b[1;31m# Update mask to ignore seen indices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "losses = []\n",
    "\n",
    "for i_epoch, epoch in enumerate(range(params['nof_epoch'])):\n",
    "    batch_loss = []\n",
    "    iterator = tqdm(dataloader, unit='Batch')\n",
    "    \n",
    "    for i_batch, sample_batched in enumerate(iterator):\n",
    "        iterator.set_description('Epoch %i/%i' % (epoch+1, params['nof_epoch']))\n",
    "\n",
    "        x, y, chars = sample_batched\n",
    "        train_batch = Variable(x)\n",
    "        target_batch = Variable(y)\n",
    "\n",
    "        if USE_CUDA:\n",
    "            train_batch = train_batch.cuda()\n",
    "            target_batch = target_batch.cuda()\n",
    "\n",
    "        o, p = model(train_batch)\n",
    "        o = o.contiguous().view(-1, o.size()[-1])\n",
    "        target_batch = target_batch.view(-1)\n",
    "        \n",
    "        loss = CCE(o, target_batch) #/ target_batch.shape[1] # need to take the length of the table into account\n",
    "        #acc = get_accuracy(p, target_batch)\n",
    "        \n",
    "        losses.append(loss.data)\n",
    "        batch_loss.append(loss.data)\n",
    "\n",
    "        model_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        model_optim.step()\n",
    "        \n",
    "        iterator.set_postfix(loss='{}'.format(loss.data))\n",
    "    batch_loss = torch.Tensor(batch_loss)\n",
    "    iterator.set_postfix(loss=np.average(batch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PointerNet(\n",
       "  (embedding): Linear(in_features=300, out_features=300, bias=True)\n",
       "  (encoder): Encoder(\n",
       "    (lstm): LSTM(300, 512, num_layers=8, dropout=0.3)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (input_to_hidden): Linear(in_features=300, out_features=2048, bias=True)\n",
       "    (hidden_to_hidden): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (hidden_out): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (att): Attention(\n",
       "      (input_linear): Linear(in_features=512, out_features=512, bias=True)\n",
       "      (context_linear): Conv1d(512, 512, kernel_size=(1,), stride=(1,))\n",
       "      (tanh): Tanh()\n",
       "      (softmax): Softmax(dim=None)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y, z = dataset[:100]\n",
    "x = x.cuda()\n",
    "y = y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "o, p = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorting = list(zip(z, p.data.cpu().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['n', 't', 'y', 'q'], [0, 3, 1, 2]),\n",
       " (['o', 'q', 's', 'j'], [3, 0, 1, 2]),\n",
       " (['q', 'l', 'r', 'x'], [1, 0, 2, 3]),\n",
       " (['j', 'e', 'p', 'y'], [1, 0, 2, 3]),\n",
       " (['u', 'k', 'o', 'h'], [3, 1, 2, 0]),\n",
       " (['y', 'g', 'u', 'x'], [1, 2, 3, 0]),\n",
       " (['e', 'n', 'l', 'f'], [0, 3, 2, 1]),\n",
       " (['s', 'k', 't', 'v'], [1, 0, 2, 3]),\n",
       " (['r', 'r', 'e', 'd'], [3, 2, 0, 1]),\n",
       " (['b', 'x', 'a', 'z'], [0, 2, 1, 3]),\n",
       " (['g', 't', 'y', 'e'], [3, 0, 1, 2]),\n",
       " (['k', 'h', 'p', 'o'], [1, 0, 3, 2]),\n",
       " (['v', 'b', 'h', 'w'], [1, 2, 0, 3]),\n",
       " (['d', 'f', 'n', 'u'], [0, 1, 2, 3]),\n",
       " (['g', 'b', 'j', 'o'], [1, 0, 2, 3]),\n",
       " (['e', 'b', 'b', 't'], [1, 2, 0, 3]),\n",
       " (['h', 'c', 'i', 'z'], [1, 0, 2, 3]),\n",
       " (['y', 's', 'l', 'e'], [3, 2, 1, 0]),\n",
       " (['h', 'w', 'x', 'u'], [0, 3, 1, 2]),\n",
       " (['h', 's', 'm', 'v'], [0, 2, 1, 3]),\n",
       " (['x', 'r', 'k', 'p'], [2, 3, 1, 0]),\n",
       " (['d', 'x', 'q', 's'], [0, 2, 3, 1]),\n",
       " (['j', 'n', 'r', 'c'], [3, 0, 1, 2]),\n",
       " (['l', 'd', 'o', 'l'], [1, 0, 3, 2]),\n",
       " (['d', 'o', 'f', 'm'], [0, 2, 3, 1]),\n",
       " (['m', 'f', 'c', 'a'], [3, 2, 1, 0]),\n",
       " (['t', 'o', 'c', 'n'], [2, 1, 3, 0]),\n",
       " (['b', 'a', 'g', 'j'], [1, 0, 2, 3]),\n",
       " (['r', 'u', 'p', 'i'], [3, 2, 0, 1]),\n",
       " (['k', 'z', 'c', 'x'], [2, 0, 3, 1]),\n",
       " (['b', 'u', 'm', 'l'], [0, 3, 2, 1]),\n",
       " (['s', 'u', 'x', 'j'], [3, 0, 1, 2]),\n",
       " (['y', 'o', 'h', 'd'], [3, 2, 1, 0]),\n",
       " (['o', 'e', 'x', 'e'], [1, 3, 0, 2]),\n",
       " (['u', 'e', 'c', 'b'], [3, 2, 1, 0]),\n",
       " (['s', 'n', 'i', 'p'], [2, 1, 3, 0]),\n",
       " (['t', 'l', 'a', 'z'], [2, 1, 0, 3]),\n",
       " (['j', 'v', 'm', 'b'], [3, 0, 2, 1]),\n",
       " (['e', 'f', 'r', 'h'], [0, 1, 3, 2]),\n",
       " (['u', 'a', 'f', 'h'], [1, 2, 3, 0]),\n",
       " (['q', 'j', 'u', 'e'], [3, 1, 0, 2]),\n",
       " (['t', 'g', 'w', 'k'], [1, 3, 0, 2]),\n",
       " (['x', 's', 'e', 'm'], [2, 3, 1, 0]),\n",
       " (['j', 'l', 'j', 'm'], [0, 2, 1, 3]),\n",
       " (['q', 'u', 'j', 'p'], [2, 3, 0, 1]),\n",
       " (['u', 'd', 'v', 'a'], [3, 1, 0, 2]),\n",
       " (['x', 'v', 'k', 'r'], [2, 3, 1, 0]),\n",
       " (['r', 'f', 'v', 'd'], [3, 1, 0, 2]),\n",
       " (['e', 'c', 'l', 'y'], [1, 0, 2, 3]),\n",
       " (['l', 'z', 'r', 'b'], [3, 0, 2, 1]),\n",
       " (['h', 'z', 'e', 'n'], [2, 0, 3, 1]),\n",
       " (['z', 'x', 'm', 'a'], [3, 2, 1, 0]),\n",
       " (['d', 'j', 'e', 'r'], [0, 2, 1, 3]),\n",
       " (['g', 'h', 'm', 'k'], [0, 1, 3, 2]),\n",
       " (['l', 'h', 'c', 'v'], [2, 1, 0, 3]),\n",
       " (['w', 'l', 'i', 'v'], [2, 1, 3, 0]),\n",
       " (['t', 't', 'p', 'w'], [2, 0, 1, 3]),\n",
       " (['d', 't', 'a', 'w'], [2, 0, 1, 3]),\n",
       " (['f', 'f', 'h', 'r'], [0, 1, 2, 3]),\n",
       " (['w', 'l', 'w', 'j'], [3, 1, 0, 2]),\n",
       " (['o', 'h', 'd', 'i'], [2, 1, 3, 0]),\n",
       " (['o', 't', 'o', 'c'], [3, 0, 2, 1]),\n",
       " (['y', 'o', 'w', 'q'], [1, 3, 2, 0]),\n",
       " (['b', 'u', 'd', 'd'], [0, 3, 2, 1]),\n",
       " (['y', 'm', 'f', 'q'], [2, 1, 3, 0]),\n",
       " (['b', 'h', 'z', 's'], [0, 1, 3, 2]),\n",
       " (['k', 'a', 'q', 'b'], [1, 3, 0, 2]),\n",
       " (['b', 'x', 'n', 'f'], [0, 3, 2, 1]),\n",
       " (['c', 'k', 'o', 'a'], [3, 0, 1, 2]),\n",
       " (['h', 'n', 'r', 's'], [0, 1, 2, 3]),\n",
       " (['w', 'g', 'x', 'b'], [3, 1, 0, 2]),\n",
       " (['e', 'a', 'f', 'p'], [1, 0, 2, 3]),\n",
       " (['o', 'o', 'q', 'r'], [0, 1, 2, 3]),\n",
       " (['m', 'r', 'u', 'j'], [3, 0, 1, 2]),\n",
       " (['e', 'd', 'v', 'w'], [1, 0, 2, 3]),\n",
       " (['h', 'k', 'j', 'l'], [0, 2, 1, 3]),\n",
       " (['h', 'd', 'd', 'h'], [1, 2, 0, 3]),\n",
       " (['z', 'o', 'q', 'i'], [3, 1, 2, 0]),\n",
       " (['c', 'w', 'c', 'p'], [0, 2, 3, 1]),\n",
       " (['j', 'f', 'c', 'd'], [2, 3, 1, 0]),\n",
       " (['m', 'a', 'o', 'b'], [1, 3, 0, 2]),\n",
       " (['h', 'b', 'l', 'q'], [1, 0, 2, 3]),\n",
       " (['c', 'h', 'w', 'i'], [0, 1, 3, 2]),\n",
       " (['j', 'c', 'j', 'l'], [1, 0, 2, 3]),\n",
       " (['j', 'b', 'k', 'k'], [1, 0, 2, 3]),\n",
       " (['q', 'o', 'c', 'j'], [2, 3, 1, 0]),\n",
       " (['v', 'l', 'w', 'b'], [3, 1, 0, 2]),\n",
       " (['a', 's', 'y', 'w'], [0, 1, 3, 2]),\n",
       " (['o', 'l', 'w', 'p'], [1, 0, 3, 2]),\n",
       " (['i', 'b', 'z', 'n'], [1, 0, 3, 2]),\n",
       " (['v', 'm', 'd', 'v'], [2, 1, 0, 3]),\n",
       " (['h', 'r', 'l', 'd'], [3, 0, 2, 1]),\n",
       " (['m', 'c', 'a', 'f'], [2, 1, 3, 0]),\n",
       " (['c', 'n', 'h', 'd'], [0, 3, 2, 1]),\n",
       " (['e', 'p', 'm', 'k'], [0, 3, 2, 1]),\n",
       " (['b', 'h', 'u', 'f'], [0, 3, 1, 2]),\n",
       " (['z', 'j', 'l', 's'], [1, 2, 3, 0]),\n",
       " (['r', 'n', 'f', 'v'], [2, 1, 0, 3]),\n",
       " (['o', 'x', 'z', 'l'], [3, 0, 1, 2]),\n",
       " (['r', 'i', 'i', 'h'], [3, 1, 2, 0])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n', 'y', 'q', 't']  -  ['n', 'q', 't', 'y']\n",
      "['q', 's', 'j', 'o']  -  ['j', 'o', 'q', 's']\n",
      "['l', 'q', 'r', 'x']  -  ['l', 'q', 'r', 'x']\n",
      "['e', 'j', 'p', 'y']  -  ['e', 'j', 'p', 'y']\n",
      "['h', 'k', 'o', 'u']  -  ['h', 'k', 'o', 'u']\n",
      "['x', 'y', 'g', 'u']  -  ['g', 'u', 'x', 'y']\n",
      "['e', 'f', 'l', 'n']  -  ['e', 'f', 'l', 'n']\n",
      "['k', 's', 't', 'v']  -  ['k', 's', 't', 'v']\n",
      "['e', 'd', 'r']  -  ['d', 'e', 'r', 'r']\n",
      "['b', 'a', 'x', 'z']  -  ['a', 'b', 'x', 'z']\n",
      "['t', 'y', 'e', 'g']  -  ['e', 'g', 't', 'y']\n",
      "['h', 'k', 'o', 'p']  -  ['h', 'k', 'o', 'p']\n",
      "['h', 'v', 'b', 'w']  -  ['b', 'h', 'v', 'w']\n",
      "['d', 'f', 'n', 'u']  -  ['d', 'f', 'n', 'u']\n",
      "['b', 'g', 'j', 'o']  -  ['b', 'g', 'j', 'o']\n",
      "['b', 'e', 't']  -  ['b', 'b', 'e', 't']\n",
      "['c', 'h', 'i', 'z']  -  ['c', 'h', 'i', 'z']\n",
      "['e', 'l', 's', 'y']  -  ['e', 'l', 's', 'y']\n",
      "['h', 'x', 'u', 'w']  -  ['h', 'u', 'w', 'x']\n",
      "['h', 'm', 's', 'v']  -  ['h', 'm', 's', 'v']\n",
      "['p', 'k', 'x', 'r']  -  ['k', 'p', 'r', 'x']\n",
      "['d', 's', 'x', 'q']  -  ['d', 'q', 's', 'x']\n",
      "['n', 'r', 'c', 'j']  -  ['c', 'j', 'n', 'r']\n",
      "['d', 'l', 'o']  -  ['d', 'l', 'l', 'o']\n",
      "['d', 'm', 'o', 'f']  -  ['d', 'f', 'm', 'o']\n",
      "['a', 'c', 'f', 'm']  -  ['a', 'c', 'f', 'm']\n",
      "['n', 'o', 't', 'c']  -  ['c', 'n', 'o', 't']\n",
      "['a', 'b', 'g', 'j']  -  ['a', 'b', 'g', 'j']\n",
      "['p', 'i', 'u', 'r']  -  ['i', 'p', 'r', 'u']\n",
      "['z', 'x', 'k', 'c']  -  ['c', 'k', 'x', 'z']\n",
      "['b', 'l', 'm', 'u']  -  ['b', 'l', 'm', 'u']\n",
      "['u', 'x', 'j', 's']  -  ['j', 's', 'u', 'x']\n",
      "['d', 'h', 'o', 'y']  -  ['d', 'h', 'o', 'y']\n",
      "['x', 'o', 'e']  -  ['e', 'e', 'o', 'x']\n",
      "['b', 'c', 'e', 'u']  -  ['b', 'c', 'e', 'u']\n",
      "['p', 'n', 's', 'i']  -  ['i', 'n', 'p', 's']\n",
      "['a', 'l', 't', 'z']  -  ['a', 'l', 't', 'z']\n",
      "['v', 'b', 'm', 'j']  -  ['b', 'j', 'm', 'v']\n",
      "['e', 'f', 'h', 'r']  -  ['e', 'f', 'h', 'r']\n",
      "['h', 'u', 'a', 'f']  -  ['a', 'f', 'h', 'u']\n",
      "['u', 'j', 'e', 'q']  -  ['e', 'j', 'q', 'u']\n",
      "['w', 't', 'k', 'g']  -  ['g', 'k', 't', 'w']\n",
      "['m', 'e', 'x', 's']  -  ['e', 'm', 's', 'x']\n",
      "['j', 'l', 'm']  -  ['j', 'j', 'l', 'm']\n",
      "['j', 'p', 'q', 'u']  -  ['j', 'p', 'q', 'u']\n",
      "['v', 'd', 'a', 'u']  -  ['a', 'd', 'u', 'v']\n",
      "['r', 'k', 'x', 'v']  -  ['k', 'r', 'v', 'x']\n",
      "['v', 'f', 'd', 'r']  -  ['d', 'f', 'r', 'v']\n",
      "['c', 'e', 'l', 'y']  -  ['c', 'e', 'l', 'y']\n",
      "['z', 'b', 'r', 'l']  -  ['b', 'l', 'r', 'z']\n",
      "['z', 'n', 'h', 'e']  -  ['e', 'h', 'n', 'z']\n",
      "['a', 'm', 'x', 'z']  -  ['a', 'm', 'x', 'z']\n",
      "['d', 'e', 'j', 'r']  -  ['d', 'e', 'j', 'r']\n",
      "['g', 'h', 'k', 'm']  -  ['g', 'h', 'k', 'm']\n",
      "['c', 'h', 'l', 'v']  -  ['c', 'h', 'l', 'v']\n",
      "['v', 'l', 'w', 'i']  -  ['i', 'l', 'v', 'w']\n",
      "['t', 'p', 'w']  -  ['p', 't', 't', 'w']\n",
      "['t', 'a', 'd', 'w']  -  ['a', 'd', 't', 'w']\n",
      "['f', 'h', 'r']  -  ['f', 'f', 'h', 'r']\n",
      "['w', 'l', 'j']  -  ['j', 'l', 'w', 'w']\n",
      "['i', 'h', 'o', 'd']  -  ['d', 'h', 'i', 'o']\n",
      "['t', 'c', 'o']  -  ['c', 'o', 'o', 't']\n",
      "['q', 'y', 'w', 'o']  -  ['o', 'q', 'w', 'y']\n",
      "['b', 'd', 'u']  -  ['b', 'd', 'd', 'u']\n",
      "['q', 'm', 'y', 'f']  -  ['f', 'm', 'q', 'y']\n",
      "['b', 'h', 's', 'z']  -  ['b', 'h', 's', 'z']\n",
      "['q', 'k', 'b', 'a']  -  ['a', 'b', 'k', 'q']\n",
      "['b', 'f', 'n', 'x']  -  ['b', 'f', 'n', 'x']\n",
      "['k', 'o', 'a', 'c']  -  ['a', 'c', 'k', 'o']\n",
      "['h', 'n', 'r', 's']  -  ['h', 'n', 'r', 's']\n",
      "['x', 'g', 'b', 'w']  -  ['b', 'g', 'w', 'x']\n",
      "['a', 'e', 'f', 'p']  -  ['a', 'e', 'f', 'p']\n",
      "['o', 'q', 'r']  -  ['o', 'o', 'q', 'r']\n",
      "['r', 'u', 'j', 'm']  -  ['j', 'm', 'r', 'u']\n",
      "['d', 'e', 'v', 'w']  -  ['d', 'e', 'v', 'w']\n",
      "['h', 'j', 'k', 'l']  -  ['h', 'j', 'k', 'l']\n",
      "['d', 'h']  -  ['d', 'd', 'h', 'h']\n",
      "['i', 'o', 'q', 'z']  -  ['i', 'o', 'q', 'z']\n",
      "['p', 'w', 'c']  -  ['c', 'c', 'p', 'w']\n",
      "['d', 'c', 'j', 'f']  -  ['c', 'd', 'f', 'j']\n",
      "['o', 'm', 'b', 'a']  -  ['a', 'b', 'm', 'o']\n",
      "['b', 'h', 'l', 'q']  -  ['b', 'h', 'l', 'q']\n",
      "['c', 'h', 'i', 'w']  -  ['c', 'h', 'i', 'w']\n",
      "['c', 'j', 'l']  -  ['c', 'j', 'j', 'l']\n",
      "['b', 'j', 'k']  -  ['b', 'j', 'k', 'k']\n",
      "['j', 'c', 'q', 'o']  -  ['c', 'j', 'o', 'q']\n",
      "['w', 'l', 'b', 'v']  -  ['b', 'l', 'v', 'w']\n",
      "['a', 's', 'w', 'y']  -  ['a', 's', 'w', 'y']\n",
      "['l', 'o', 'p', 'w']  -  ['l', 'o', 'p', 'w']\n",
      "['b', 'i', 'n', 'z']  -  ['b', 'i', 'n', 'z']\n",
      "['d', 'm', 'v']  -  ['d', 'm', 'v', 'v']\n",
      "['r', 'd', 'l', 'h']  -  ['d', 'h', 'l', 'r']\n",
      "['f', 'c', 'm', 'a']  -  ['a', 'c', 'f', 'm']\n",
      "['c', 'd', 'h', 'n']  -  ['c', 'd', 'h', 'n']\n",
      "['e', 'k', 'm', 'p']  -  ['e', 'k', 'm', 'p']\n",
      "['b', 'u', 'f', 'h']  -  ['b', 'f', 'h', 'u']\n",
      "['s', 'z', 'j', 'l']  -  ['j', 'l', 's', 'z']\n",
      "['f', 'n', 'r', 'v']  -  ['f', 'n', 'r', 'v']\n",
      "['x', 'z', 'l', 'o']  -  ['l', 'o', 'x', 'z']\n",
      "['h', 'i', 'r']  -  ['h', 'i', 'i', 'r']\n"
     ]
    }
   ],
   "source": [
    "for pair in sorting:\n",
    "    sequence, pointers = pair\n",
    "    tmp = {}\n",
    "    for seq, p in zip(sequence, pointers):\n",
    "        tmp[seq] = p\n",
    "    tmp =  [k for k, v in sorted(tmp.items(), key=lambda item: item[1])]\n",
    "    print(tmp, \" - \", sorted(sequence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
