{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mögliche Labels:\n",
    "- eindeutige Positionierung der Zielspalte\n",
    "- mehrfache Positionierungen der Zielspalten sortiert nach derer Kosinus-Ähnlichkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import WordEmbedding, load_word_emb\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_config = {\n",
    "    'data_dir': 'data/glove',\n",
    "    'word2idx_path': 'word2idx.json',\n",
    "    'usedwordemb_path': 'usedwordemb.npy'\n",
    "}\n",
    "w2v = WordEmbedding(load_word_emb(w2v_config['data_dir'], \n",
    "                                  w2v_config['word2idx_path'],\n",
    "                                  w2v_config['usedwordemb_path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>header</th>\n",
       "      <th>fits_1toN</th>\n",
       "      <th>fits_1to0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Date, Time, ACC Team, Big Ten Team, Location,...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Institution, Wins, Loss, Home Wins, Home Loss...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Pick #, MLS Team, Player, Position, Affiliation]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[DVD title, Number of Episodes, Region 2, Regi...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Year, Coach, Crew, Record, Win %]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              header fits_1toN fits_1to0\n",
       "0  [Date, Time, ACC Team, Big Ten Team, Location,...                    \n",
       "1  [Institution, Wins, Loss, Home Wins, Home Loss...                    \n",
       "2  [Pick #, MLS Team, Player, Position, Affiliation]                    \n",
       "3  [DVD title, Number of Episodes, Region 2, Regi...                    \n",
       "4                 [Year, Coach, Crew, Record, Win %]                    "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load table schemata\n",
    "with open('data/wikisql/tables.jsonl') as file:\n",
    "    table_schemata = pd.DataFrame([json.loads(line) for line in file.readlines()])\n",
    "table_schemata['fits_1toN'] = ''\n",
    "table_schemata['fits_1to0'] = ''\n",
    "table_schemata['header'] = table_schemata['header'].apply(lambda x: '<|>'.join(x)) # needed to drop duplicates\n",
    "table_schemata = table_schemata[['header', 'fits_1toN', 'fits_1to0']].drop_duplicates().reset_index(drop=True)\n",
    "table_schemata['header'] = table_schemata['header'].apply(lambda x: x.split('<|>')) # rebuild original state\n",
    "table_schemata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table_schemata = table_schemata.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headers = []\n",
    "_ = [all_headers.extend(header) for header in table_schemata['header'].values]\n",
    "all_headers = list(set(all_headers))\n",
    "random.shuffle(all_headers)\n",
    "candidates = [column \\\n",
    "              .replace('/', ' ') \\\n",
    "              .replace('_', ' ') \\\n",
    "              for column in all_headers[:5000]]\n",
    "cache = {candidate: np.mean([w2v(word.lower()) for word in candidate.split()], axis=0) for candidate in candidates}\n",
    "if cache.get(''):\n",
    "    del cache['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 9899/9899 [4:06:27<00:00,  1.49s/it]\n"
     ]
    }
   ],
   "source": [
    "def calculate_1to1(header, cache, threshold=0.8):\n",
    "    eps = 0.001\n",
    "    embs = [np.mean([w2v(word.lower()) for word in col.split()], axis=0) for col in header]\n",
    "    fits = defaultdict(list)\n",
    "    for candidate, embedding in cache.items():\n",
    "        try:\n",
    "            vectors = [embedding] + embs\n",
    "            similarity = cosine_similarity(vectors)[0][1:]\n",
    "            max_sim = np.max(similarity)\n",
    "            if abs(max_sim + eps) >= 1.0 or max_sim < threshold:\n",
    "                continue\n",
    "            max_pos = np.argmax(similarity)\n",
    "\n",
    "            # append the best candidate with its similarity to the header\n",
    "            # to the lists of candidates for the given header\n",
    "            # i.e. fits['ACC Team'] -> [('Team', 0.75), ('Coach', 0.64), (<new candidate>, <similarity>)]\n",
    "            fits[header[max_pos]].append((candidate, max_sim))\n",
    "        except:\n",
    "            continue\n",
    "    return fits    \n",
    "\n",
    "def calculate_1t0(header, cache, threshold=0.6):\n",
    "    embs = [np.mean([w2v(word.lower()) for word in col.split()], axis=0) for col in header]\n",
    "    fits = defaultdict(list)\n",
    "    for candidate, embedding in cache.items():\n",
    "        try:\n",
    "            vectors = [embedding] + embs\n",
    "            similarity = cosine_similarity(vectors)[0][1:]\n",
    "            max_sim = np.max(similarity)\n",
    "            # if the candidate is not similar to any of the columns, append it to the fits\n",
    "            if max_sim < threshold:\n",
    "                min_sim = np.min(similarity)\n",
    "                min_pos = np.argmin(similarity)\n",
    "                fits[header[min_pos]].append((candidate, min_sim))\n",
    "        except:\n",
    "            continue\n",
    "    return fits\n",
    "\n",
    "def reduce_fits(good_fits, increase_similarity=True, lower_threshold=0.9, upper_threshold=0.3):\n",
    "    best_fits = {}\n",
    "    for column, candidate_tuples in good_fits.items():\n",
    "        best_candidate = sorted(candidate_tuples, key=lambda x: x[1], reverse=True if increase_similarity else False)[0]\n",
    "        similarity = best_candidate[1]\n",
    "        if increase_similarity:\n",
    "            if similarity > lower_threshold:\n",
    "                best_fits[column] = best_candidate[0]\n",
    "        else:\n",
    "            if similarity < upper_threshold:\n",
    "                best_fits[column] = best_candidate[0]\n",
    "    return best_fits\n",
    "\n",
    "def expand_targets(header, cache, fits_1to1, threshold=0.7):\n",
    "    fits_1toN = fits_1to1.copy()\n",
    "    for source_col, target_col in fits_1to1.items():\n",
    "        other_cols = header[:header.index(target_col)] + header[header.index(target_col) + 1:]\n",
    "        embs = [np.mean([w2v(word.lower()) for word in col.split()], axis=0) for col in other_cols]\n",
    "        vectors = embs + [cache[source_col]]\n",
    "        similarity = cosine_similarity(vectors)[0][1:]\n",
    "        indexed_similarity = sorted(enumerate(similarity), key=lambda item: item[1], reverse=True)\n",
    "        for idx, sim in indexed_similarity:\n",
    "            if sim > threshold:\n",
    "                fits_1toN[source_col] += '<|>{}'.format(other_cols[idx])\n",
    "    return fits_1toN\n",
    "\n",
    "for idx, row in tqdm(table_schemata.iterrows(), total=len(table_schemata)):\n",
    "    header = row['header']\n",
    "    fits_1to1 = calculate_1to1(header, cache)\n",
    "    fits_1to1 = reduce_fits(fits_1to1, increase_similarity=True)\n",
    "    \n",
    "    fits_1to0 = calculate_1t0(header, cache)\n",
    "    fits_1to0 = reduce_fits(fits_1to0, increase_similarity=False)\n",
    "    \n",
    "    # swap order from target_col:sourc_col to source_col: target_col\n",
    "    # implementation originally done in the target_col:source_col format\n",
    "    # as this is easier to handle for candidate reduction\n",
    "    fits_1to1 = {value: key for key, value in fits_1to1.items()}\n",
    "    fits_1to0 = {value: key for key, value in fits_1to0.items()}\n",
    "    \n",
    "    fits_1toN = expand_targets(header, cache, fits_1to1, threshold=0.7)\n",
    "    \n",
    "    row['fits_1toN'] = fits_1toN\n",
    "    row['fits_1to0'] = fits_1to0\n",
    "    table_schemata.loc[idx] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_col</th>\n",
       "      <th>input_cols</th>\n",
       "      <th>target_cols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Year Ended</td>\n",
       "      <td>Year&lt;|&gt;Champion&lt;|&gt;Score&lt;|&gt;Runner-up&lt;|&gt;Coach</td>\n",
       "      <td>Year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>% Of the population (Censo 2010)</td>\n",
       "      <td>Contestant&lt;|&gt;Starting Weight (kg)&lt;|&gt;Final Weig...</td>\n",
       "      <td>Position (out of Eliminated Contestants)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Area in acres (ha)</td>\n",
       "      <td>English name&lt;|&gt;Original name&lt;|&gt;Area in km²&lt;|&gt;P...</td>\n",
       "      <td>Area in km²&lt;|&gt;English name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Example name</td>\n",
       "      <td>Name&lt;|&gt;Country&lt;|&gt;Status&lt;|&gt;Transfer window&lt;|&gt;Tr...</td>\n",
       "      <td>Name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xDSL</td>\n",
       "      <td>1st throw&lt;|&gt;2nd throw&lt;|&gt;3rd throw&lt;|&gt;Equation&lt;|...</td>\n",
       "      <td>&lt;NONE&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         source_col  \\\n",
       "0                        Year Ended   \n",
       "1  % Of the population (Censo 2010)   \n",
       "2                Area in acres (ha)   \n",
       "3                      Example name   \n",
       "4                              xDSL   \n",
       "\n",
       "                                          input_cols  \\\n",
       "0        Year<|>Champion<|>Score<|>Runner-up<|>Coach   \n",
       "1  Contestant<|>Starting Weight (kg)<|>Final Weig...   \n",
       "2  English name<|>Original name<|>Area in km²<|>P...   \n",
       "3  Name<|>Country<|>Status<|>Transfer window<|>Tr...   \n",
       "4  1st throw<|>2nd throw<|>3rd throw<|>Equation<|...   \n",
       "\n",
       "                                target_cols  \n",
       "0                                      Year  \n",
       "1  Position (out of Eliminated Contestants)  \n",
       "2                Area in km²<|>English name  \n",
       "3                                      Name  \n",
       "4                                    <NONE>  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_schemata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'fits_1toN'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pointer_sort\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2896\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2897\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'fits_1toN'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-cd7312b91fe6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Drop rows that have not been tested\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m table_schemata.drop(table_schemata.loc[(table_schemata['fits_1toN'] == '') & \\\n\u001b[0m\u001b[0;32m      3\u001b[0m                                        (table_schemata['fits_1to0'] == '')].index, inplace=True)\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pointer_sort\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2993\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2994\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2995\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2996\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\pointer_sort\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2897\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2898\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2899\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2901\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'fits_1toN'"
     ]
    }
   ],
   "source": [
    "# Drop rows that have not been tested\n",
    "table_schemata.drop(table_schemata.loc[(table_schemata['fits_1toN'] == '') & \\\n",
    "                                       (table_schemata['fits_1to0'] == '')].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9899it [00:05, 1976.35it/s]\n"
     ]
    }
   ],
   "source": [
    "rows_1toN = []\n",
    "rows_1to0 = []\n",
    "total_rows = []\n",
    "ratio_1to0_1toN = 0.5 # relative ratio of how many 1to0 cases exist in comparison to 1toN cases\n",
    "for idx, ts_row in tqdm(table_schemata.iterrows()):\n",
    "    header, fits_1toN = ts_row[['header', 'fits_1toN']]\n",
    "    num_words_in_header = len((' '.join(header)).split(' '))\n",
    "    if num_words_in_header > 30:\n",
    "        continue\n",
    "    for source_col, target_cols in fits_1toN.items(): \n",
    "        if 'k {\\math' in source_col:\n",
    "            continue\n",
    "        seq_row = {\n",
    "            'source_col': source_col,\n",
    "            'input_cols': '<|>'.join(header),\n",
    "            'target_cols': '<|>'.join(target_cols.split('<|>')) # first create a list from the joined \n",
    "                        # target cols, then concatenate them (needs to be done to serialize them correctly)\n",
    "        }\n",
    "        rows_1toN.append(seq_row)\n",
    "    for source_col, _ in fits_1to0.items(): \n",
    "        if 'k {\\math' in source_col:\n",
    "            continue\n",
    "        seq_row = {\n",
    "            'source_col': source_col,\n",
    "            'input_cols': '<|>'.join(header),\n",
    "            'target_cols': '<NONE>'\n",
    "        }\n",
    "        rows_1to0.append(seq_row)\n",
    "amount_1to0 = int(len(rows_1toN) * ratio_1to0_1toN)\n",
    "rows_1toN, rows_1to0 = shuffle(rows_1toN), shuffle(rows_1to0)[:amount_1to0]\n",
    "\n",
    "total_rows = shuffle(rows_1toN + rows_1to0)\n",
    "\n",
    "total_rows = pd.DataFrame(total_rows)\n",
    "total_rows.to_csv('data/training/schema_matching_raw_1toN.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_col</th>\n",
       "      <th>input_cols</th>\n",
       "      <th>target_cols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>High School</td>\n",
       "      <td>School&lt;|&gt;Nickname&lt;|&gt;Location&lt;|&gt;Colors&lt;|&gt;Type&lt;|...</td>\n",
       "      <td>School</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008 Rank</td>\n",
       "      <td>April 2013 Cum. Rank&lt;|&gt;Name&lt;|&gt;Rank 2012&lt;|&gt;Rank...</td>\n",
       "      <td>Rank 2012&lt;|&gt;Name&lt;|&gt;2013 Profit (mil. USD )&lt;|&gt;M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Last Current driver(s) September 22, 2013</td>\n",
       "      <td>Country&lt;|&gt;Total Drivers&lt;|&gt;Champions&lt;|&gt;Champion...</td>\n",
       "      <td>Last/Current driver(s) 3 November 2013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Last year</td>\n",
       "      <td>Year&lt;|&gt;Conductor&lt;|&gt;Opera House or Orchestra&lt;|&gt;...</td>\n",
       "      <td>Year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mdns</td>\n",
       "      <td>Provider&lt;|&gt;Country&lt;|&gt;Subscribers (2005) (thous...</td>\n",
       "      <td>&lt;NONE&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  source_col  \\\n",
       "0                                High School   \n",
       "1                                  2008 Rank   \n",
       "2  Last Current driver(s) September 22, 2013   \n",
       "3                                  Last year   \n",
       "4                                       Mdns   \n",
       "\n",
       "                                          input_cols  \\\n",
       "0  School<|>Nickname<|>Location<|>Colors<|>Type<|...   \n",
       "1  April 2013 Cum. Rank<|>Name<|>Rank 2012<|>Rank...   \n",
       "2  Country<|>Total Drivers<|>Champions<|>Champion...   \n",
       "3  Year<|>Conductor<|>Opera House or Orchestra<|>...   \n",
       "4  Provider<|>Country<|>Subscribers (2005) (thous...   \n",
       "\n",
       "                                         target_cols  \n",
       "0                                             School  \n",
       "1  Rank 2012<|>Name<|>2013 Profit (mil. USD )<|>M...  \n",
       "2             Last/Current driver(s) 3 November 2013  \n",
       "3                                               Year  \n",
       "4                                             <NONE>  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_rows.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6569, 8240, 4899)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_1to0 = len(total_rows[total_rows['target_cols'] == '<NONE>'])\n",
    "num_1toN = len(total_rows[(total_rows['target_cols'].str.contains('<|>')) &\n",
    "                          (total_rows['target_cols'] != '<NONE>')])\n",
    "num_1to1 = len(total_rows) - num_1toN - num_1to0\n",
    "num_1to0, num_1to1, num_1toN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19708, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_rows.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorgehen:\n",
    "1. für jede Tabelle eine Spalte finden, die aus Quellschema sein könnte -> die als Eingabe  \n",
    "Format: Quellspalte -> [Zielspalte]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
