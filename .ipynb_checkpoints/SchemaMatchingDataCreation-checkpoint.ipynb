{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mögliche Labels:\n",
    "- eindeutige Positionierung der Zielspalte\n",
    "- mehrfache Positionierungen der Zielspalten sortiert nach derer Kosinus-Ähnlichkeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocessing import WordEmbedding, load_word_emb\n",
    "import random\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_config = {\n",
    "    'data_dir': 'data/glove',\n",
    "    'word2idx_path': 'word2idx.json',\n",
    "    'usedwordemb_path': 'usedwordemb.npy'\n",
    "}\n",
    "w2v = WordEmbedding(load_word_emb(w2v_config['data_dir'], \n",
    "                                  w2v_config['word2idx_path'],\n",
    "                                  w2v_config['usedwordemb_path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>header</th>\n",
       "      <th>fits_1toN</th>\n",
       "      <th>fits_1to0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Date, Time, ACC Team, Big Ten Team, Location,...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Institution, Wins, Loss, Home Wins, Home Loss...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Pick #, MLS Team, Player, Position, Affiliation]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[DVD title, Number of Episodes, Region 2, Regi...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Year, Coach, Crew, Record, Win %]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              header fits_1toN fits_1to0\n",
       "0  [Date, Time, ACC Team, Big Ten Team, Location,...                    \n",
       "1  [Institution, Wins, Loss, Home Wins, Home Loss...                    \n",
       "2  [Pick #, MLS Team, Player, Position, Affiliation]                    \n",
       "3  [DVD title, Number of Episodes, Region 2, Regi...                    \n",
       "4                 [Year, Coach, Crew, Record, Win %]                    "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load table schemata\n",
    "with open('data/wikisql/tables.jsonl') as file:\n",
    "    table_schemata = pd.DataFrame([json.loads(line) for line in file.readlines()])\n",
    "table_schemata['fits_1toN'] = ''\n",
    "table_schemata['fits_1to0'] = ''\n",
    "table_schemata['header'] = table_schemata['header'].apply(lambda x: '<|>'.join(x)) # needed to drop duplicates\n",
    "table_schemata = table_schemata[['header', 'fits_1toN', 'fits_1to0']].drop_duplicates().reset_index(drop=True)\n",
    "table_schemata['header'] = table_schemata['header'].apply(lambda x: x.split('<|>')) # rebuild original state\n",
    "table_schemata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#table_schemata = table_schemata.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_headers = []\n",
    "_ = [all_headers.extend(header) for header in table_schemata['header'].values]\n",
    "all_headers = list(set(all_headers))\n",
    "random.shuffle(all_headers)\n",
    "candidates = [column \\\n",
    "              .replace('/', ' ') \\\n",
    "              .replace('_', ' ') \\\n",
    "              for column in all_headers[:12000]]\n",
    "cache = {candidate: np.mean([w2v(word.lower()) for word in candidate.split()], axis=0) for candidate in candidates}\n",
    "if cache.get(''):\n",
    "    del cache['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 9899/9899 [8:58:34<00:00,  3.26s/it]\n"
     ]
    }
   ],
   "source": [
    "def calculate_1to1(header, cache, threshold=0.8):\n",
    "    eps = 0.001\n",
    "    embs = [np.mean([w2v(word.lower()) for word in col.split()], axis=0) for col in header]\n",
    "    fits = defaultdict(list)\n",
    "    for candidate, embedding in cache.items():\n",
    "        try:\n",
    "            vectors = [embedding] + embs\n",
    "            similarity = cosine_similarity(vectors)[0][1:]\n",
    "            max_sim = np.max(similarity)\n",
    "            if abs(max_sim + eps) >= 1.0 or max_sim < threshold:\n",
    "                continue\n",
    "            max_pos = np.argmax(similarity)\n",
    "\n",
    "            # append the best candidate with its similarity to the header\n",
    "            # to the lists of candidates for the given header\n",
    "            # i.e. fits['ACC Team'] -> [('Team', 0.75), ('Coach', 0.64), (<new candidate>, <similarity>)]\n",
    "            fits[header[max_pos]].append((candidate, max_sim))\n",
    "        except:\n",
    "            continue\n",
    "    return fits    \n",
    "\n",
    "def calculate_1t0(header, cache, threshold=0.6):\n",
    "    embs = [np.mean([w2v(word.lower()) for word in col.split()], axis=0) for col in header]\n",
    "    fits = defaultdict(list)\n",
    "    for candidate, embedding in cache.items():\n",
    "        try:\n",
    "            vectors = [embedding] + embs\n",
    "            similarity = cosine_similarity(vectors)[0][1:]\n",
    "            max_sim = np.max(similarity)\n",
    "            # if the candidate is not similar to any of the columns, append it to the fits\n",
    "            if max_sim < threshold:\n",
    "                min_sim = np.min(similarity)\n",
    "                min_pos = np.argmin(similarity)\n",
    "                fits[header[min_pos]].append((candidate, min_sim))\n",
    "        except:\n",
    "            continue\n",
    "    return fits\n",
    "\n",
    "def reduce_fits(good_fits, increase_similarity=True, lower_threshold=0.9, upper_threshold=0.3):\n",
    "    best_fits = {}\n",
    "    for column, candidate_tuples in good_fits.items():\n",
    "        best_candidate = sorted(candidate_tuples, key=lambda x: x[1], reverse=True if increase_similarity else False)[0]\n",
    "        similarity = best_candidate[1]\n",
    "        if increase_similarity:\n",
    "            if similarity > lower_threshold:\n",
    "                best_fits[column] = best_candidate[0]\n",
    "        else:\n",
    "            if similarity < upper_threshold:\n",
    "                best_fits[column] = best_candidate[0]\n",
    "    return best_fits\n",
    "\n",
    "def expand_targets(header, cache, fits_1to1, threshold=0.7):\n",
    "    fits_1toN = fits_1to1.copy()\n",
    "    for source_col, target_col in fits_1to1.items():\n",
    "        other_cols = header[:header.index(target_col)] + header[header.index(target_col) + 1:]\n",
    "        embs = [np.mean([w2v(word.lower()) for word in col.split()], axis=0) for col in other_cols]\n",
    "        vectors = embs + [cache[source_col]]\n",
    "        similarity = cosine_similarity(vectors)[0][1:]\n",
    "        indexed_similarity = sorted(enumerate(similarity), key=lambda item: item[1], reverse=True)\n",
    "        for idx, sim in indexed_similarity:\n",
    "            if sim > threshold:\n",
    "                fits_1toN[source_col] += '<|>{}'.format(other_cols[idx])\n",
    "    return fits_1toN\n",
    "\n",
    "for idx, row in tqdm(table_schemata.iterrows(), total=len(table_schemata)):\n",
    "    header = row['header']\n",
    "    fits_1to1 = calculate_1to1(header, cache)\n",
    "    fits_1to1 = reduce_fits(fits_1to1, increase_similarity=True)\n",
    "    \n",
    "    fits_1to0 = calculate_1t0(header, cache)\n",
    "    fits_1to0 = reduce_fits(fits_1to0, increase_similarity=False)\n",
    "    \n",
    "    # swap order from target_col:sourc_col to source_col: target_col\n",
    "    # implementation originally done in the target_col:source_col format\n",
    "    # as this is easier to handle for candidate reduction\n",
    "    fits_1to1 = {value: key for key, value in fits_1to1.items()}\n",
    "    fits_1to0 = {value: key for key, value in fits_1to0.items()}\n",
    "    \n",
    "    fits_1toN = expand_targets(header, cache, fits_1to1, threshold=0.75)\n",
    "    \n",
    "    row['fits_1toN'] = fits_1toN\n",
    "    row['fits_1to0'] = fits_1to0\n",
    "    table_schemata.loc[idx] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>header</th>\n",
       "      <th>fits_1toN</th>\n",
       "      <th>fits_1to0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Date, Time, ACC Team, Big Ten Team, Location,...</td>\n",
       "      <td>{'Big Ten': 'Big Ten Team', 'Time   Time Zone'...</td>\n",
       "      <td>{'hppa': 'Attendance', 'Blks': 'Television', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Institution, Wins, Loss, Home Wins, Home Loss...</td>\n",
       "      <td>{'Loss gain': 'Loss'}</td>\n",
       "      <td>{'1993-2001': 'Wins', 'Genestealers': 'Institu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Pick #, MLS Team, Player, Position, Affiliation]</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'IntelliTrace': 'Affiliation', 'Co-singer': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[DVD title, Number of Episodes, Region 2, Regi...</td>\n",
       "      <td>{'Region 4': 'Region 2', 'Region 2 (Germany)':...</td>\n",
       "      <td>{'Karianne Gulliksen': 'DVD title', 'Qual.': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Year, Coach, Crew, Record, Win %]</td>\n",
       "      <td>{'Last year': 'Year', '% Won': 'Win %'}</td>\n",
       "      <td>{'Mundubbera': 'Coach', '7800.00': 'Record', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              header  \\\n",
       "0  [Date, Time, ACC Team, Big Ten Team, Location,...   \n",
       "1  [Institution, Wins, Loss, Home Wins, Home Loss...   \n",
       "2  [Pick #, MLS Team, Player, Position, Affiliation]   \n",
       "3  [DVD title, Number of Episodes, Region 2, Regi...   \n",
       "4                 [Year, Coach, Crew, Record, Win %]   \n",
       "\n",
       "                                           fits_1toN  \\\n",
       "0  {'Big Ten': 'Big Ten Team', 'Time   Time Zone'...   \n",
       "1                              {'Loss gain': 'Loss'}   \n",
       "2                                                 {}   \n",
       "3  {'Region 4': 'Region 2', 'Region 2 (Germany)':...   \n",
       "4            {'Last year': 'Year', '% Won': 'Win %'}   \n",
       "\n",
       "                                           fits_1to0  \n",
       "0  {'hppa': 'Attendance', 'Blks': 'Television', '...  \n",
       "1  {'1993-2001': 'Wins', 'Genestealers': 'Institu...  \n",
       "2  {'IntelliTrace': 'Affiliation', 'Co-singer': '...  \n",
       "3  {'Karianne Gulliksen': 'DVD title', 'Qual.': '...  \n",
       "4  {'Mundubbera': 'Coach', '7800.00': 'Record', '...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_schemata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows that have not been tested\n",
    "table_schemata.drop(table_schemata.loc[(table_schemata['fits_1toN'] == '') & \\\n",
    "                                       (table_schemata['fits_1to0'] == '')].index, inplace=True)\n",
    "table_schemata.to_csv('data/training/table_schemata.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>header</th>\n",
       "      <th>fits_1toN</th>\n",
       "      <th>fits_1to0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Date, Time, ACC Team, Big Ten Team, Location,...</td>\n",
       "      <td>{'Big Ten': 'Big Ten Team', 'Time   Time Zone'...</td>\n",
       "      <td>{'hppa': 'Attendance', 'Blks': 'Television', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[Institution, Wins, Loss, Home Wins, Home Loss...</td>\n",
       "      <td>{'Loss gain': 'Loss'}</td>\n",
       "      <td>{'1993-2001': 'Wins', 'Genestealers': 'Institu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[Pick #, MLS Team, Player, Position, Affiliation]</td>\n",
       "      <td>{}</td>\n",
       "      <td>{'IntelliTrace': 'Affiliation', 'Co-singer': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[DVD title, Number of Episodes, Region 2, Regi...</td>\n",
       "      <td>{'Region 4': 'Region 2', 'Region 2 (Germany)':...</td>\n",
       "      <td>{'Karianne Gulliksen': 'DVD title', 'Qual.': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Year, Coach, Crew, Record, Win %]</td>\n",
       "      <td>{'Last year': 'Year', '% Won': 'Win %'}</td>\n",
       "      <td>{'Mundubbera': 'Coach', '7800.00': 'Record', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              header  \\\n",
       "0  [Date, Time, ACC Team, Big Ten Team, Location,...   \n",
       "1  [Institution, Wins, Loss, Home Wins, Home Loss...   \n",
       "2  [Pick #, MLS Team, Player, Position, Affiliation]   \n",
       "3  [DVD title, Number of Episodes, Region 2, Regi...   \n",
       "4                 [Year, Coach, Crew, Record, Win %]   \n",
       "\n",
       "                                           fits_1toN  \\\n",
       "0  {'Big Ten': 'Big Ten Team', 'Time   Time Zone'...   \n",
       "1                              {'Loss gain': 'Loss'}   \n",
       "2                                                 {}   \n",
       "3  {'Region 4': 'Region 2', 'Region 2 (Germany)':...   \n",
       "4            {'Last year': 'Year', '% Won': 'Win %'}   \n",
       "\n",
       "                                           fits_1to0  \n",
       "0  {'hppa': 'Attendance', 'Blks': 'Television', '...  \n",
       "1  {'1993-2001': 'Wins', 'Genestealers': 'Institu...  \n",
       "2  {'IntelliTrace': 'Affiliation', 'Co-singer': '...  \n",
       "3  {'Karianne Gulliksen': 'DVD title', 'Qual.': '...  \n",
       "4  {'Mundubbera': 'Coach', '7800.00': 'Record', '...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_schemata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9899it [00:04, 2037.84it/s]\n"
     ]
    }
   ],
   "source": [
    "rows_1toN = []\n",
    "rows_1to0 = []\n",
    "total_rows = []\n",
    "ratio_1to0_1toN = 0.3 # relative ratio of how many 1to0 cases exist in comparison to 1toN cases\n",
    "for idx, ts_row in tqdm(table_schemata.iterrows()):\n",
    "    header, fits_1toN, fits_1to0 = ts_row[['header', 'fits_1toN', 'fits_1to0']]\n",
    "    num_words_in_header = len((' '.join(header)).split(' '))\n",
    "    #if num_words_in_header > 30:\n",
    "    #    continue\n",
    "    for source_col, target_cols in fits_1toN.items(): \n",
    "        if 'k {\\math' in source_col:\n",
    "            continue\n",
    "        seq_row = {\n",
    "            'source_col': source_col,\n",
    "            'input_cols': '<|>'.join(header),\n",
    "            'target_cols': '<|>'.join(target_cols.split('<|>')) # first create a list from the joined \n",
    "                        # target cols, then concatenate them (needs to be done to serialize them correctly)\n",
    "        }\n",
    "        rows_1toN.append(seq_row)\n",
    "    for source_col, _ in fits_1to0.items(): \n",
    "        if 'k {\\math' in source_col:\n",
    "            continue\n",
    "        seq_row = {\n",
    "            'source_col': source_col,\n",
    "            'input_cols': '<|>'.join(header),\n",
    "            'target_cols': '<NONE>'\n",
    "        }\n",
    "        rows_1to0.append(seq_row)\n",
    "amount_1to0 = int(len(rows_1toN) * ratio_1to0_1toN)\n",
    "rows_1toN, rows_1to0 = shuffle(rows_1toN), shuffle(rows_1to0)[:amount_1to0]\n",
    "\n",
    "total_rows = shuffle(rows_1toN + rows_1to0)\n",
    "\n",
    "total_rows = pd.DataFrame(total_rows)\n",
    "total_rows.to_csv('data/training/schema_matching_raw_1toN.txt', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6433 18023 3421\n"
     ]
    }
   ],
   "source": [
    "rows_1to0 = total_rows[total_rows['target_cols'] == '<NONE>']\n",
    "rows_1toN = total_rows[(total_rows['target_cols'].str.contains('<|>')) &\n",
    "                          (total_rows['target_cols'] != '<NONE>')]\n",
    "rows_1to1 = total_rows.drop(rows_1to0.index).drop(rows_1toN.index)\n",
    "num_1to0, num_1to1, num_1toN = len(rows_1to0), len(rows_1to1), len(rows_1toN)\n",
    "print(num_1to0, num_1to1, num_1toN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_1to0 = rows_1to0.sample(frac=1).reset_index(drop=True)[:3000]\n",
    "rows_1to1 = rows_1to1.sample(frac=1).reset_index(drop=True)[:9000]\n",
    "rows_1toN = rows_1toN.sample(frac=1).reset_index(drop=True)[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_col</th>\n",
       "      <th>input_cols</th>\n",
       "      <th>target_cols</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Season Premiere Date</td>\n",
       "      <td>Season&lt;|&gt;Episodes&lt;|&gt;Time slot (EST)&lt;|&gt;Season p...</td>\n",
       "      <td>Season premiere&lt;|&gt;Time slot (EST)&lt;|&gt;Season fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Week 12 Nov 23</td>\n",
       "      <td>Week 9 Oct 29&lt;|&gt;Week 10 Nov 5&lt;|&gt;Week 11 Nov 12...</td>\n",
       "      <td>Week 12 Nov 19&lt;|&gt;Week 11 Nov 12&lt;|&gt;Week 15 (Fin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2009 2010</td>\n",
       "      <td>2001&lt;|&gt;2004&lt;|&gt;2009&lt;|&gt;2013&lt;|&gt;Total</td>\n",
       "      <td>2009&lt;|&gt;2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Week 12</td>\n",
       "      <td>Week 1&lt;|&gt;Week 2&lt;|&gt;Week 3&lt;|&gt;Week 4&lt;|&gt;Week 5&lt;|&gt;W...</td>\n",
       "      <td>Week 8&lt;|&gt;Week 1&lt;|&gt;Week 2&lt;|&gt;Week 3&lt;|&gt;Week 5&lt;|&gt;W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Percent of votes</td>\n",
       "      <td>Election&lt;|&gt;Number of PNC votes&lt;|&gt;Share of vote...</td>\n",
       "      <td>Share of votes&lt;|&gt;Seats</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3416</th>\n",
       "      <td>Platform(s) [A ]</td>\n",
       "      <td>STAPLE:&lt;|&gt;Maize / Corn [A ]&lt;|&gt;Rice [B ]&lt;|&gt;Whea...</td>\n",
       "      <td>Soybean (Green) [F ]&lt;|&gt;Yam [Y ]&lt;|&gt;Sweet potato...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3417</th>\n",
       "      <td>Conservative ticket</td>\n",
       "      <td>Republican ticket&lt;|&gt;Democratic ticket&lt;|&gt;Americ...</td>\n",
       "      <td>Republican ticket&lt;|&gt;Socialist ticket&lt;|&gt;Communi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3418</th>\n",
       "      <td>Order number</td>\n",
       "      <td>Model Number&lt;|&gt;Frequency&lt;|&gt;L2-Cache&lt;|&gt;Multipli...</td>\n",
       "      <td>Order Part Number&lt;|&gt;Release date</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3419</th>\n",
       "      <td>Original Canadian air date</td>\n",
       "      <td>No. in series&lt;|&gt;No. in season&lt;|&gt;Title&lt;|&gt;Direct...</td>\n",
       "      <td>Original air date&lt;|&gt;No. in series</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3420</th>\n",
       "      <td>Time   Time Zone</td>\n",
       "      <td>Elimination number&lt;|&gt;Wrestler&lt;|&gt;Entered&lt;|&gt;Elim...</td>\n",
       "      <td>Time&lt;|&gt;Eliminated by</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3421 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      source_col  \\\n",
       "0           Season Premiere Date   \n",
       "1                 Week 12 Nov 23   \n",
       "2                      2009 2010   \n",
       "3                        Week 12   \n",
       "4               Percent of votes   \n",
       "...                          ...   \n",
       "3416            Platform(s) [A ]   \n",
       "3417         Conservative ticket   \n",
       "3418                Order number   \n",
       "3419  Original Canadian air date   \n",
       "3420            Time   Time Zone   \n",
       "\n",
       "                                             input_cols  \\\n",
       "0     Season<|>Episodes<|>Time slot (EST)<|>Season p...   \n",
       "1     Week 9 Oct 29<|>Week 10 Nov 5<|>Week 11 Nov 12...   \n",
       "2                     2001<|>2004<|>2009<|>2013<|>Total   \n",
       "3     Week 1<|>Week 2<|>Week 3<|>Week 4<|>Week 5<|>W...   \n",
       "4     Election<|>Number of PNC votes<|>Share of vote...   \n",
       "...                                                 ...   \n",
       "3416  STAPLE:<|>Maize / Corn [A ]<|>Rice [B ]<|>Whea...   \n",
       "3417  Republican ticket<|>Democratic ticket<|>Americ...   \n",
       "3418  Model Number<|>Frequency<|>L2-Cache<|>Multipli...   \n",
       "3419  No. in series<|>No. in season<|>Title<|>Direct...   \n",
       "3420  Elimination number<|>Wrestler<|>Entered<|>Elim...   \n",
       "\n",
       "                                            target_cols  \n",
       "0     Season premiere<|>Time slot (EST)<|>Season fin...  \n",
       "1     Week 12 Nov 19<|>Week 11 Nov 12<|>Week 15 (Fin...  \n",
       "2                                           2009<|>2001  \n",
       "3     Week 8<|>Week 1<|>Week 2<|>Week 3<|>Week 5<|>W...  \n",
       "4                                Share of votes<|>Seats  \n",
       "...                                                 ...  \n",
       "3416  Soybean (Green) [F ]<|>Yam [Y ]<|>Sweet potato...  \n",
       "3417  Republican ticket<|>Socialist ticket<|>Communi...  \n",
       "3418                   Order Part Number<|>Release date  \n",
       "3419                  Original air date<|>No. in series  \n",
       "3420                               Time<|>Eliminated by  \n",
       "\n",
       "[3421 rows x 3 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_1toN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_rows = pd.concat([rows_1to0, rows_1to1, rows_1toN], axis=0)\n",
    "total_rows.to_csv('data/training/schema_matching_raw_1toN_pruned.txt', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vorgehen:\n",
    "1. für jede Tabelle eine Spalte finden, die aus Quellschema sein könnte -> die als Eingabe  \n",
    "Format: Quellspalte -> [Zielspalte]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
