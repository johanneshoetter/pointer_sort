{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.datasets import AlphabetSortingDataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # Data\n",
    "    'batch_size': 2,\n",
    "    'shuffle': True\n",
    "}\n",
    "\n",
    "dataset = AlphabetSortingDataset(5, min_len=2, max_len=4)\n",
    "dataloader = DataLoader(dataset,\n",
    "                        batch_size=params['batch_size'],\n",
    "                        shuffle=params['shuffle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] -- Train\n",
      "OUT SHAPE torch.Size([32, 48, 300])\n",
      "OUTPUTS SHAPE  torch.Size([48, 32])\n",
      "DEC IN SHAPE  torch.Size([32, 1, 1])\n",
      "HS SHAPE  torch.Size([1, 32, 300]) torch.Size([1, 32, 300])\n",
      "0\n",
      "DEC IN SHAPE 2:  torch.Size([32, 300])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[32, 1, 1]' is invalid for input of size 9600",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-5aaf95b47d76>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    237\u001b[0m                         shuffle=params['shuffle'])\n\u001b[0;32m    238\u001b[0m     \u001b[0mtest_dataloader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mptr_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m     \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mptr_net\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-5aaf95b47d76>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, dataloader, optimizer, epoch, clip)\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;31m# Forward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;31m# Backward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\ca\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-5aaf95b47d76>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x, y, teacher_force_ratio)\u001b[0m\n\u001b[0;32m    167\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"DEC IN SHAPE 2: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdec_in\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;31m# TEST: mit 300 multipliziert, um embedding faktor einzubauen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 169\u001b[1;33m             \u001b[0mdec_in\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdec_in\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    170\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[1;31m# Add cross entropy loss (F.log_softmax + nll_loss)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[32, 1, 1]' is invalid for input of size 9600"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Module implementing the pointer network proposed at: https://arxiv.org/abs/1506.03134\n",
    "The implementation try to follows the variables naming conventions\n",
    "ei: Encoder hidden state\n",
    "di: Decoder hidden state\n",
    "di_prime: Attention aware decoder state\n",
    "W1: Learnable matrix (Attention layer)\n",
    "W2: Learnable matrix (Attention layer)\n",
    "V: Learnable parameter (Attention layer)\n",
    "uj: Energy vector (Attention Layer)\n",
    "aj: Attention mask over the input\n",
    "\"\"\"\n",
    "import random\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from data import batch\n",
    "\n",
    "HIDDEN_SIZE = 300\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "STEPS_PER_EPOCH = 500\n",
    "EPOCHS = 100\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_size: int):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x: (BATCH, ARRAY_LEN, 1)\n",
    "        return self.lstm(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size, units):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W1 = nn.Linear(hidden_size, units, bias=False)\n",
    "        self.W2 = nn.Linear(hidden_size, units, bias=False)\n",
    "        self.V =  nn.Linear(units, 1, bias=False)\n",
    "\n",
    "    def forward(self, \n",
    "                  encoder_out: torch.Tensor, \n",
    "                  decoder_hidden: torch.Tensor):\n",
    "        # encoder_out: (BATCH, ARRAY_LEN, HIDDEN_SIZE)\n",
    "        # decoder_hidden: (BATCH, HIDDEN_SIZE)\n",
    "\n",
    "        # Add time axis to decoder hidden state\n",
    "        # in order to make operations compatible with encoder_out\n",
    "        # decoder_hidden_time: (BATCH, 1, HIDDEN_SIZE)\n",
    "        decoder_hidden_time = decoder_hidden.unsqueeze(1)\n",
    "\n",
    "        # uj: (BATCH, ARRAY_LEN, ATTENTION_UNITS)\n",
    "        # Note: we can add the both linear outputs thanks to broadcasting\n",
    "        uj = self.W1(encoder_out) + self.W2(decoder_hidden_time)\n",
    "        uj = torch.tanh(uj)\n",
    "\n",
    "        # uj: (BATCH, ARRAY_LEN, 1)\n",
    "        uj = self.V(uj)\n",
    "\n",
    "        # Attention mask over inputs\n",
    "        # aj: (BATCH, ARRAY_LEN, 1)\n",
    "        aj = F.softmax(uj, dim=1)\n",
    "\n",
    "        # di_prime: (BATCH, HIDDEN_SIZE)\n",
    "        di_prime = aj * encoder_out\n",
    "        di_prime = di_prime.sum(1)\n",
    "\n",
    "        return di_prime, uj.squeeze(-1)\n",
    "    \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, \n",
    "               hidden_size: int,\n",
    "               attention_units: int = 10):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.lstm = nn.LSTM(hidden_size + 1, hidden_size, batch_first=True)\n",
    "        self.attention = Attention(hidden_size, attention_units)\n",
    "\n",
    "    def forward(self, \n",
    "                  x: torch.Tensor, \n",
    "                  hidden: Tuple[torch.Tensor], \n",
    "                  encoder_out: torch.Tensor):\n",
    "        # x: (BATCH, 1, 1) \n",
    "        # hidden: (1, BATCH, HIDDEN_SIZE)\n",
    "        # encoder_out: (BATCH, ARRAY_LEN, HIDDEN_SIZE)\n",
    "        # For a better understanding about hidden shapes read: https://pytorch.org/docs/stable/nn.html#lstm\n",
    "\n",
    "        # Get hidden states (not cell states) \n",
    "        # from the first and unique LSTM layer \n",
    "        ht = hidden[0][0]  # ht: (BATCH, HIDDEN_SIZE)\n",
    "\n",
    "        # di: Attention aware hidden state -> (BATCH, HIDDEN_SIZE)\n",
    "        # att_w: Not 'softmaxed', torch will take care of it -> (BATCH, ARRAY_LEN)\n",
    "        di, att_w = self.attention(encoder_out, ht)\n",
    "\n",
    "        # Append attention aware hidden state to our input\n",
    "        # x: (BATCH, 1, 1 + HIDDEN_SIZE)\n",
    "        \n",
    "        # Test: ohne unsqueeze\n",
    "        x = torch.cat([di.unsqueeze(1), x], dim=2)\n",
    "        #x = torch.cat([di, x], dim=2)\n",
    "\n",
    "        # Generate the hidden state for next timestep\n",
    "        _, hidden = self.lstm(x, hidden)\n",
    "        return hidden, att_w\n",
    "\n",
    "\n",
    "class PointerNetwork(nn.Module):\n",
    "    def __init__(self, \n",
    "                   encoder: nn.Module, \n",
    "                   decoder: nn.Module):\n",
    "        super(PointerNetwork, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, \n",
    "                  x: torch.Tensor, \n",
    "                  y: torch.Tensor, \n",
    "                  teacher_force_ratio=.5):\n",
    "        # x: (BATCH_SIZE, ARRAY_LEN)\n",
    "        # y: (BATCH_SIZE, ARRAY_LEN)\n",
    "\n",
    "        # Array elements as features\n",
    "        # encoder_in: (BATCH, ARRAY_LEN, 1)\n",
    "        \n",
    "        # TEST: ohne unsqueeze\n",
    "        #encoder_in = x.unsqueeze(-1).type(torch.float)\n",
    "        encoder_in = x.type(torch.float)\n",
    "        \n",
    "        # out: (BATCH, ARRAY_LEN, HIDDEN_SIZE)\n",
    "        # hs: tuple of (NUM_LAYERS, BATCH, HIDDEN_SIZE)\n",
    "        out, hs = encoder(encoder_in)\n",
    "        \n",
    "        print(\"OUT SHAPE\", out.shape)\n",
    "\n",
    "        # Accum loss throughout timesteps\n",
    "        loss = 0\n",
    "\n",
    "        # Save outputs at each timestep\n",
    "        # outputs: (ARRAY_LEN, BATCH)\n",
    "        outputs = torch.zeros(out.size(1), out.size(0), dtype=torch.long)\n",
    "        print(\"OUTPUTS SHAPE \", outputs.shape)\n",
    "\n",
    "        # First decoder input is always 0\n",
    "        # dec_in: (BATCH, 1, 1)\n",
    "        dec_in = torch.zeros(out.size(0), 1, 1, dtype=torch.float)\n",
    "        print(\"DEC IN SHAPE \", dec_in.shape)\n",
    "        print(\"HS SHAPE \" , hs[0].shape, hs[1].shape)\n",
    "\n",
    "        for t in range(out.size(1)):\n",
    "            # for each time step\n",
    "            print(t)\n",
    "            hs, att_w = decoder(dec_in, hs, out)\n",
    "            predictions = F.softmax(att_w, dim=1).argmax(1)\n",
    "\n",
    "            # Pick next index\n",
    "            # If teacher force the next element will we the ground truth\n",
    "            # otherwise will be the predicted value at current timestep\n",
    "            teacher_force = random.random() < teacher_force_ratio\n",
    "            idx = y[:, t] if teacher_force else predictions\n",
    "            dec_in = torch.stack([x[b, idx[b].item()] for b in range(x.size(0))])\n",
    "            print(\"DEC IN SHAPE 2: \", dec_in.shape)\n",
    "            # TEST: mit 300 multipliziert, um embedding faktor einzubauen\n",
    "            dec_in = dec_in.view(out.size(0), 1, 1).type(torch.float)\n",
    "\n",
    "            # Add cross entropy loss (F.log_softmax + nll_loss)\n",
    "            loss += F.cross_entropy(att_w, y[:, t])\n",
    "            outputs[t] = predictions\n",
    "\n",
    "        # Weight losses, so every element in the batch \n",
    "        # has the same 'importance' \n",
    "        batch_loss = loss / y.size(0)\n",
    "\n",
    "        return outputs, batch_loss\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, epoch, clip=1.):\n",
    "    \"\"\"Train single epoch\"\"\"\n",
    "    print('Epoch [{}] -- Train'.format(epoch))\n",
    "    for idx, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward\n",
    "        x, y = batch\n",
    "        out, loss = model(x, y)\n",
    "\n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print('Epoch [{}] loss: {}'.format(epoch, loss.item()))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, dataloader, epoch):\n",
    "    \"\"\"Evaluate after a train epoch\"\"\"\n",
    "    print('Epoch [{}] -- Evaluate'.format(epoch))\n",
    "\n",
    "    x_val, y_val = next(iter(dataloader))\n",
    "\n",
    "    out, _ = model(x_val, y_val, teacher_force_ratio=0.)\n",
    "    out = out.permute(1, 0)\n",
    "\n",
    "    for i in range(out.size(0)):\n",
    "        print('{} --> {} --> {}'.format(\n",
    "          x_val[i], \n",
    "          x_val[i].gather(0, out[i]), \n",
    "          x_val[i].gather(0, y_val[i])\n",
    "        ))\n",
    "\n",
    "\n",
    "encoder = Encoder(HIDDEN_SIZE)\n",
    "decoder = Decoder(HIDDEN_SIZE)\n",
    "ptr_net = PointerNetwork(encoder, decoder)\n",
    "\n",
    "optimizer = optim.Adam(ptr_net.parameters())\n",
    "\n",
    "\n",
    "NUM_SAMPLES = 100\n",
    "params = {\n",
    "    # Data\n",
    "    'batch_size': 32,\n",
    "    'shuffle': True\n",
    "}\n",
    "for epoch in range(EPOCHS):\n",
    "    train_dataset = AlphabetSortingDataset(NUM_SAMPLES)\n",
    "    test_dataset = AlphabetSortingDataset(4)\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                        batch_size=params['batch_size'],\n",
    "                        shuffle=params['shuffle'])\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True)\n",
    "    train(ptr_net, train_dataloader, optimizer, epoch + 1)\n",
    "    evaluate(ptr_net, test_dataloader, epoch + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden size auf embedding size gesetzt -> muss angepasst werden!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
